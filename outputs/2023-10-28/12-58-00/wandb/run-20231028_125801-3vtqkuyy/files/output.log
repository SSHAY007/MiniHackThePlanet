[[36m2023-10-28 12:58:05,066[39m][[34mroot[39m][[32mINFO[39m] - Logging results to /workspace/outputs/2023-10-28/12-58-00
[[36m2023-10-28 12:58:05,094[39m][[34mpalaas/out[39m][[32mINFO[39m] - Found log directory: /workspace/outputs/2023-10-28/12-58-00
[[36m2023-10-28 12:58:05,095[39m][[34mpalaas/out[39m][[32mINFO[39m] - Saving arguments to /workspace/outputs/2023-10-28/12-58-00/meta.json
[[36m2023-10-28 12:58:05,095[39m][[34mpalaas/out[39m][[32mINFO[39m] - Saving messages to /workspace/outputs/2023-10-28/12-58-00/out.log
[[36m2023-10-28 12:58:05,096[39m][[34mpalaas/out[39m][[32mINFO[39m] - Saving logs data to /workspace/outputs/2023-10-28/12-58-00/logs.csv
[[36m2023-10-28 12:58:05,096[39m][[34mpalaas/out[39m][[32mINFO[39m] - Saving logs' fields to /workspace/outputs/2023-10-28/12-58-00/fields.csv
[[36m2023-10-28 12:58:05,096[39m][[34mroot[39m][[32mINFO[39m] - Not using CUDA.
[[36m2023-10-28 12:58:05,102[39m][[34mroot[39m][[32mINFO[39m] - Using model hks
[[36m2023-10-28 12:58:05,131[39m][[34mroot[39m][[32mINFO[39m] - ('/workspace/skill_transfer_weights/mini_skill_pick_up.tar', '/workspace/skill_transfer_weights/skill_config.yaml')
[[36m2023-10-28 12:58:05,131[39m][[34mroot[39m][[32mINFO[39m] - ('/workspace/skill_transfer_weights/mini_skill_fight.tar', '/workspace/skill_transfer_weights/skill_config.yaml')
[[36m2023-10-28 12:58:05,131[39m][[34mroot[39m][[32mINFO[39m] - ('/workspace/skill_transfer_weights/mini_skill_nav_blind.tar', '/workspace/skill_transfer_weights/skill_config.yaml')
[[36m2023-10-28 12:58:05,131[39m][[34mroot[39m][[32mINFO[39m] - ('/workspace/skill_transfer_weights/mini_skill_nav_lava.tar', '/workspace/skill_transfer_weights/skill_config.yaml')
[[36m2023-10-28 12:58:05,316[39m][[34mroot[39m][[32mINFO[39m] - Number of model parameters: 4244546
[[36m2023-10-28 12:58:05,339[39m][[34mroot[39m][[32mINFO[39m] - ('/workspace/skill_transfer_weights/mini_skill_pick_up.tar', '/workspace/skill_transfer_weights/skill_config.yaml')
[[36m2023-10-28 12:58:05,340[39m][[34mroot[39m][[32mINFO[39m] - ('/workspace/skill_transfer_weights/mini_skill_fight.tar', '/workspace/skill_transfer_weights/skill_config.yaml')
[[36m2023-10-28 12:58:05,340[39m][[34mroot[39m][[32mINFO[39m] - ('/workspace/skill_transfer_weights/mini_skill_nav_blind.tar', '/workspace/skill_transfer_weights/skill_config.yaml')
[[36m2023-10-28 12:58:05,340[39m][[34mroot[39m][[32mINFO[39m] - ('/workspace/skill_transfer_weights/mini_skill_nav_lava.tar', '/workspace/skill_transfer_weights/skill_config.yaml')
Found log directory: /workspace/outputs/2023-10-28/12-58-00
Saving arguments to /workspace/outputs/2023-10-28/12-58-00/meta.json
Saving messages to /workspace/outputs/2023-10-28/12-58-00/out.log
Saving logs data to /workspace/outputs/2023-10-28/12-58-00/logs.csv
Saving logs' fields to /workspace/outputs/2023-10-28/12-58-00/fields.csv
[[36m2023-10-28 12:58:10,796[39m][[34mroot[39m][[32mINFO[39m] - Step 0 @ 0.0 SPS. Inference batcher size: 100. Learner queue size: 0. Other stats: (train_seconds = 5.0)
[[36m2023-10-28 12:58:15,803[39m][[34mroot[39m][[32mINFO[39m] - Step 0 @ 0.0 SPS. Inference batcher size: 128. Learner queue size: 0. Other stats: (train_seconds = 10.0)
[[36m2023-10-28 12:58:20,808[39m][[34mroot[39m][[32mINFO[39m] - Step 0 @ 0.0 SPS. Inference batcher size: 151. Learner queue size: 0. Other stats: (train_seconds = 15.0)
[[36m2023-10-28 12:58:25,813[39m][[34mroot[39m][[32mINFO[39m] - Step 0 @ 0.0 SPS. Inference batcher size: 128. Learner queue size: 0. Other stats: (train_seconds = 20.0)
[[36m2023-10-28 12:58:30,820[39m][[34mroot[39m][[32mINFO[39m] - Step 0 @ 0.0 SPS. Inference batcher size: 118. Learner queue size: 0. Other stats: (train_seconds = 25.0)
[[36m2023-10-28 12:58:35,823[39m][[34mroot[39m][[32mINFO[39m] - Step 0 @ 0.0 SPS. Inference batcher size: 72. Learner queue size: 0. Other stats: (train_seconds = 30.0)
[[36m2023-10-28 12:58:40,828[39m][[34mroot[39m][[32mINFO[39m] - Step 0 @ 0.0 SPS. Inference batcher size: 84. Learner queue size: 0. Other stats: (train_seconds = 35.0)
[[36m2023-10-28 12:58:45,836[39m][[34mroot[39m][[32mINFO[39m] - Step 0 @ 0.0 SPS. Inference batcher size: 122. Learner queue size: 0. Other stats: (train_seconds = 40.0)
[[36m2023-10-28 12:58:50,841[39m][[34mroot[39m][[32mINFO[39m] - Step 0 @ 0.0 SPS. Inference batcher size: 131. Learner queue size: 0. Other stats: (train_seconds = 45.1)
[[36m2023-10-28 12:58:55,843[39m][[34mroot[39m][[32mINFO[39m] - Step 0 @ 0.0 SPS. Inference batcher size: 69. Learner queue size: 0. Other stats: (train_seconds = 50.1)
[[36m2023-10-28 12:59:00,847[39m][[34mroot[39m][[32mINFO[39m] - Step 0 @ 0.0 SPS. Inference batcher size: 91. Learner queue size: 0. Other stats: (train_seconds = 55.1)
[[36m2023-10-28 12:59:05,852[39m][[34mroot[39m][[32mINFO[39m] - Step 0 @ 0.0 SPS. Inference batcher size: 64. Learner queue size: 0. Other stats: (train_seconds = 60.1)
[[36m2023-10-28 12:59:10,855[39m][[34mroot[39m][[32mINFO[39m] - Step 0 @ 0.0 SPS. Inference batcher size: 90. Learner queue size: 0. Other stats: (train_seconds = 65.1)
[[36m2023-10-28 12:59:15,860[39m][[34mroot[39m][[32mINFO[39m] - Step 0 @ 0.0 SPS. Inference batcher size: 143. Learner queue size: 0. Other stats: (train_seconds = 70.1)
[[36m2023-10-28 12:59:20,868[39m][[34mroot[39m][[32mINFO[39m] - Step 0 @ 0.0 SPS. Inference batcher size: 85. Learner queue size: 4. Other stats: (train_seconds = 75.1)
[[36m2023-10-28 12:59:25,872[39m][[34mroot[39m][[32mINFO[39m] - Step 0 @ 0.0 SPS. Inference batcher size: 52. Learner queue size: 4. Other stats: (train_seconds = 80.1)
[[36m2023-10-28 12:59:30,877[39m][[34mroot[39m][[32mINFO[39m] - Step 0 @ 0.0 SPS. Inference batcher size: 113. Learner queue size: 25. Other stats: (train_seconds = 85.1)
[[36m2023-10-28 12:59:35,879[39m][[34mroot[39m][[32mINFO[39m] - Step 0 @ 0.0 SPS. Inference batcher size: 41. Learner queue size: 32. Other stats: (train_seconds = 90.1)
[[36m2023-10-28 12:59:40,884[39m][[34mroot[39m][[32mINFO[39m] - Step 0 @ 0.0 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 95.1)
~ 2560 total_loss tensor(-300.1510, grad_fn=<AddBackward0>) ks_loss tensor(1.0674, grad_fn=<MulBackward0>)
### 2560 total_loss tensor(-299.0836, grad_fn=<AddBackward0>) hks_uniform_loss tensor(0.4027, grad_fn=<MulBackward0>)
[[36m2023-10-28 12:59:45,891[39m][[34mroot[39m][[32mINFO[39m] - Step 0 @ 0.0 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (train_seconds = 100.1)
Updated log fields: ['_tick', '_time', 'train_seconds', 'success_rate', 'meta_entropy', 'hks_loss', 'step', 'mean_episode_return', 'mean_episode_step', 'total_loss', 'entropy_loss', 'pg_loss', 'baseline_loss', 'learner_queue_size']
[[36m2023-10-28 12:59:50,896[39m][[34mroot[39m][[32mINFO[39m] - Step 0 @ 0.0 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (train_seconds = 105.1)
Invalid runs, some did not end in a win or a loss:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
[[36m2023-10-28 12:59:50,958[39m][[34mpalaas/out[39m][[32mINFO[39m] - Updated log fields: ['_tick', '_time', 'train_seconds', 'success_rate', 'meta_entropy', 'hks_loss', 'step', 'mean_episode_return', 'mean_episode_step', 'total_loss', 'entropy_loss', 'pg_loss', 'baseline_loss', 'learner_queue_size']
[[36m2023-10-28 12:59:55,899[39m][[34mroot[39m][[32mINFO[39m] - Saving checkpoint to /workspace/outputs/2023-10-28/12-58-00/quest_easy_0.tar
[[36m2023-10-28 12:59:55,973[39m][[34mroot[39m][[32mINFO[39m] - Step 2560 @ 511.6 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (train_seconds = 110.1, success_rate = 0.0, meta_entropy = tensor(1.3857), hks_loss = 0.4027, step = 2560, mean_episode_return = -0.0057619, mean_episode_step = 16.939, total_loss = -298.68, entropy_loss = -4.12, pg_loss = -306.88, baseline_loss = 10.848, learner_queue_size = 32, _tick = 0, _time = 1.6985e+09)
[[36m2023-10-28 13:00:00,985[39m][[34mroot[39m][[32mINFO[39m] - Step 2560 @ 0.0 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (train_seconds = 115.2, success_rate = 0.0, meta_entropy = tensor(1.3857), hks_loss = 0.4027, step = 2560, mean_episode_return = -0.0057619, mean_episode_step = 16.939, total_loss = -298.68, entropy_loss = -4.12, pg_loss = -306.88, baseline_loss = 10.848, learner_queue_size = 32, _tick = 0, _time = 1.6985e+09)
[[36m2023-10-28 13:00:05,993[39m][[34mroot[39m][[32mINFO[39m] - Step 2560 @ 0.0 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (train_seconds = 120.2, success_rate = 0.0, meta_entropy = tensor(1.3857), hks_loss = 0.4027, step = 2560, mean_episode_return = -0.0057619, mean_episode_step = 16.939, total_loss = -298.68, entropy_loss = -4.12, pg_loss = -306.88, baseline_loss = 10.848, learner_queue_size = 32, _tick = 0, _time = 1.6985e+09)
~ 5120 total_loss tensor(885.8319, grad_fn=<AddBackward0>) ks_loss tensor(0.9231, grad_fn=<MulBackward0>)
### 5120 total_loss tensor(886.7551, grad_fn=<AddBackward0>) hks_uniform_loss tensor(0.3623, grad_fn=<MulBackward0>)
[[36m2023-10-28 13:00:11,031[39m][[34mroot[39m][[32mINFO[39m] - Step 2560 @ 0.0 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 125.2, success_rate = 0.0, meta_entropy = tensor(1.3857), hks_loss = 0.4027, step = 2560, mean_episode_return = -0.0057619, mean_episode_step = 16.939, total_loss = -298.68, entropy_loss = -4.12, pg_loss = -306.88, baseline_loss = 10.848, learner_queue_size = 32, _tick = 0, _time = 1.6985e+09)
Invalid runs, some did not end in a win or a loss:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
[[36m2023-10-28 13:00:16,037[39m][[34mroot[39m][[32mINFO[39m] - Step 5120 @ 508.3 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (train_seconds = 130.2, success_rate = 0.0, meta_entropy = tensor(1.3857), hks_loss = 0.36233, step = 5120, mean_episode_return = -0.0054578, mean_episode_step = 17.601, total_loss = 887.12, entropy_loss = -4.1044, pg_loss = 814.79, baseline_loss = 75.142, learner_queue_size = 32, _tick = 1, _time = 1.6985e+09)
[[36m2023-10-28 13:00:21,046[39m][[34mroot[39m][[32mINFO[39m] - Step 5120 @ 0.0 SPS. Inference batcher size: 91. Learner queue size: 32. Other stats: (train_seconds = 135.3, success_rate = 0.0, meta_entropy = tensor(1.3857), hks_loss = 0.36233, step = 5120, mean_episode_return = -0.0054578, mean_episode_step = 17.601, total_loss = 887.12, entropy_loss = -4.1044, pg_loss = 814.79, baseline_loss = 75.142, learner_queue_size = 32, _tick = 1, _time = 1.6985e+09)
[[36m2023-10-28 13:00:26,054[39m][[34mroot[39m][[32mINFO[39m] - Step 5120 @ 0.0 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 140.3, success_rate = 0.0, meta_entropy = tensor(1.3857), hks_loss = 0.36233, step = 5120, mean_episode_return = -0.0054578, mean_episode_step = 17.601, total_loss = 887.12, entropy_loss = -4.1044, pg_loss = 814.79, baseline_loss = 75.142, learner_queue_size = 32, _tick = 1, _time = 1.6985e+09)
~ 7680 total_loss tensor(-93.2543, grad_fn=<AddBackward0>) ks_loss tensor(0.4207, grad_fn=<MulBackward0>)
### 7680 total_loss tensor(-92.8337, grad_fn=<AddBackward0>) hks_uniform_loss tensor(0.4641, grad_fn=<MulBackward0>)
[[36m2023-10-28 13:00:31,061[39m][[34mroot[39m][[32mINFO[39m] - Step 5120 @ 0.0 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (train_seconds = 145.3, success_rate = 0.0, meta_entropy = tensor(1.3857), hks_loss = 0.36233, step = 5120, mean_episode_return = -0.0054578, mean_episode_step = 17.601, total_loss = 887.12, entropy_loss = -4.1044, pg_loss = 814.79, baseline_loss = 75.142, learner_queue_size = 32, _tick = 1, _time = 1.6985e+09)
Invalid runs, some did not end in a win or a loss:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
[[36m2023-10-28 13:00:36,069[39m][[34mroot[39m][[32mINFO[39m] - Step 7680 @ 511.2 SPS. Inference batcher size: 52. Learner queue size: 32. Other stats: (train_seconds = 150.3, success_rate = 0.0, meta_entropy = tensor(1.3857), hks_loss = 0.46406, step = 7680, mean_episode_return = -0.0042674, mean_episode_step = 17.319, total_loss = -92.37, entropy_loss = -4.1142, pg_loss = -95.324, baseline_loss = 6.1839, learner_queue_size = 32, _tick = 2, _time = 1.6985e+09)
[[36m2023-10-28 13:00:41,077[39m][[34mroot[39m][[32mINFO[39m] - Step 7680 @ 0.0 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (train_seconds = 155.3, success_rate = 0.0, meta_entropy = tensor(1.3857), hks_loss = 0.46406, step = 7680, mean_episode_return = -0.0042674, mean_episode_step = 17.319, total_loss = -92.37, entropy_loss = -4.1142, pg_loss = -95.324, baseline_loss = 6.1839, learner_queue_size = 32, _tick = 2, _time = 1.6985e+09)
[[36m2023-10-28 13:00:46,082[39m][[34mroot[39m][[32mINFO[39m] - Step 7680 @ 0.0 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 160.3, success_rate = 0.0, meta_entropy = tensor(1.3857), hks_loss = 0.46406, step = 7680, mean_episode_return = -0.0042674, mean_episode_step = 17.319, total_loss = -92.37, entropy_loss = -4.1142, pg_loss = -95.324, baseline_loss = 6.1839, learner_queue_size = 32, _tick = 2, _time = 1.6985e+09)
~ 10240 total_loss tensor(343.7369, grad_fn=<AddBackward0>) ks_loss tensor(15.9998, grad_fn=<MulBackward0>)
### 10240 total_loss tensor(359.7367, grad_fn=<AddBackward0>) hks_uniform_loss tensor(10.2958, grad_fn=<MulBackward0>)
[[36m2023-10-28 13:00:51,088[39m][[34mroot[39m][[32mINFO[39m] - Step 7680 @ 0.0 SPS. Inference batcher size: 23. Learner queue size: 32. Other stats: (train_seconds = 165.3, success_rate = 0.0, meta_entropy = tensor(1.3857), hks_loss = 0.46406, step = 7680, mean_episode_return = -0.0042674, mean_episode_step = 17.319, total_loss = -92.37, entropy_loss = -4.1142, pg_loss = -95.324, baseline_loss = 6.1839, learner_queue_size = 32, _tick = 2, _time = 1.6985e+09)
Invalid runs, some did not end in a win or a loss:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
[[36m2023-10-28 13:00:56,093[39m][[34mroot[39m][[32mINFO[39m] - Step 10240 @ 511.6 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (train_seconds = 170.3, success_rate = 0.0, meta_entropy = tensor(1.3857), hks_loss = 10.296, step = 10240, mean_episode_return = -0.0051978, mean_episode_step = 14.978, total_loss = 370.03, entropy_loss = -3.9252, pg_loss = 315.35, baseline_loss = 32.313, learner_queue_size = 32, _tick = 3, _time = 1.6985e+09)
[[36m2023-10-28 13:01:01,097[39m][[34mroot[39m][[32mINFO[39m] - Step 10240 @ 0.0 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 175.3, success_rate = 0.0, meta_entropy = tensor(1.3857), hks_loss = 10.296, step = 10240, mean_episode_return = -0.0051978, mean_episode_step = 14.978, total_loss = 370.03, entropy_loss = -3.9252, pg_loss = 315.35, baseline_loss = 32.313, learner_queue_size = 32, _tick = 3, _time = 1.6985e+09)
[[36m2023-10-28 13:01:06,106[39m][[34mroot[39m][[32mINFO[39m] - Step 10240 @ 0.0 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 180.3, success_rate = 0.0, meta_entropy = tensor(1.3857), hks_loss = 10.296, step = 10240, mean_episode_return = -0.0051978, mean_episode_step = 14.978, total_loss = 370.03, entropy_loss = -3.9252, pg_loss = 315.35, baseline_loss = 32.313, learner_queue_size = 32, _tick = 3, _time = 1.6985e+09)
~ 12800 total_loss tensor(-210.6654, grad_fn=<AddBackward0>) ks_loss tensor(5.4979, grad_fn=<MulBackward0>)
### 12800 total_loss tensor(-205.1675, grad_fn=<AddBackward0>) hks_uniform_loss tensor(0.1917, grad_fn=<MulBackward0>)
[[36m2023-10-28 13:01:11,114[39m][[34mroot[39m][[32mINFO[39m] - Step 10240 @ 0.0 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (train_seconds = 185.3, success_rate = 0.0, meta_entropy = tensor(1.3857), hks_loss = 10.296, step = 10240, mean_episode_return = -0.0051978, mean_episode_step = 14.978, total_loss = 370.03, entropy_loss = -3.9252, pg_loss = 315.35, baseline_loss = 32.313, learner_queue_size = 32, _tick = 3, _time = 1.6985e+09)
Invalid runs, some did not end in a win or a loss:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
[[36m2023-10-28 13:01:16,122[39m][[34mroot[39m][[32mINFO[39m] - Step 12800 @ 511.2 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (train_seconds = 190.3, success_rate = 0.0, meta_entropy = tensor(1.3857), hks_loss = 0.19169, step = 12800, mean_episode_return = -0.0049684, mean_episode_step = 16.464, total_loss = -204.98, entropy_loss = -4.0544, pg_loss = -209.42, baseline_loss = 2.8083, learner_queue_size = 32, _tick = 4, _time = 1.6985e+09)
[[36m2023-10-28 13:01:21,130[39m][[34mroot[39m][[32mINFO[39m] - Step 12800 @ 0.0 SPS. Inference batcher size: 54. Learner queue size: 32. Other stats: (train_seconds = 195.3, success_rate = 0.0, meta_entropy = tensor(1.3857), hks_loss = 0.19169, step = 12800, mean_episode_return = -0.0049684, mean_episode_step = 16.464, total_loss = -204.98, entropy_loss = -4.0544, pg_loss = -209.42, baseline_loss = 2.8083, learner_queue_size = 32, _tick = 4, _time = 1.6985e+09)
[[36m2023-10-28 13:01:26,138[39m][[34mroot[39m][[32mINFO[39m] - Step 12800 @ 0.0 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 200.3, success_rate = 0.0, meta_entropy = tensor(1.3857), hks_loss = 0.19169, step = 12800, mean_episode_return = -0.0049684, mean_episode_step = 16.464, total_loss = -204.98, entropy_loss = -4.0544, pg_loss = -209.42, baseline_loss = 2.8083, learner_queue_size = 32, _tick = 4, _time = 1.6985e+09)
~ 15360 total_loss tensor(286.3173, grad_fn=<AddBackward0>) ks_loss tensor(0.5449, grad_fn=<MulBackward0>)
### 15360 total_loss tensor(286.8622, grad_fn=<AddBackward0>) hks_uniform_loss tensor(0.1831, grad_fn=<MulBackward0>)
[[36m2023-10-28 13:01:31,154[39m][[34mroot[39m][[32mINFO[39m] - Step 12800 @ 0.0 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (train_seconds = 205.4, success_rate = 0.0, meta_entropy = tensor(1.3857), hks_loss = 0.19169, step = 12800, mean_episode_return = -0.0049684, mean_episode_step = 16.464, total_loss = -204.98, entropy_loss = -4.0544, pg_loss = -209.42, baseline_loss = 2.8083, learner_queue_size = 32, _tick = 4, _time = 1.6985e+09)
Invalid runs, some did not end in a win or a loss:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
[[36m2023-10-28 13:01:36,163[39m][[34mroot[39m][[32mINFO[39m] - Step 15360 @ 511.1 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 210.4, success_rate = 0.0, meta_entropy = tensor(1.3851), hks_loss = 0.18309, step = 15360, mean_episode_return = -0.0053, mean_episode_step = 17.268, total_loss = 287.05, entropy_loss = -4.1125, pg_loss = 275.98, baseline_loss = 14.449, learner_queue_size = 32, _tick = 5, _time = 1.6985e+09)
[[36m2023-10-28 13:01:41,170[39m][[34mroot[39m][[32mINFO[39m] - Step 15360 @ 0.0 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (train_seconds = 215.4, success_rate = 0.0, meta_entropy = tensor(1.3851), hks_loss = 0.18309, step = 15360, mean_episode_return = -0.0053, mean_episode_step = 17.268, total_loss = 287.05, entropy_loss = -4.1125, pg_loss = 275.98, baseline_loss = 14.449, learner_queue_size = 32, _tick = 5, _time = 1.6985e+09)
[[36m2023-10-28 13:01:46,181[39m][[34mroot[39m][[32mINFO[39m] - Step 15360 @ 0.0 SPS. Inference batcher size: 44. Learner queue size: 32. Other stats: (train_seconds = 220.4, success_rate = 0.0, meta_entropy = tensor(1.3851), hks_loss = 0.18309, step = 15360, mean_episode_return = -0.0053, mean_episode_step = 17.268, total_loss = 287.05, entropy_loss = -4.1125, pg_loss = 275.98, baseline_loss = 14.449, learner_queue_size = 32, _tick = 5, _time = 1.6985e+09)
~ 17920 total_loss tensor(13.6285, grad_fn=<AddBackward0>) ks_loss tensor(1.4716, grad_fn=<MulBackward0>)
### 17920 total_loss tensor(15.1002, grad_fn=<AddBackward0>) hks_uniform_loss tensor(0.2878, grad_fn=<MulBackward0>)
[[36m2023-10-28 13:01:51,190[39m][[34mroot[39m][[32mINFO[39m] - Step 15360 @ 0.0 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (train_seconds = 225.4, success_rate = 0.0, meta_entropy = tensor(1.3851), hks_loss = 0.18309, step = 15360, mean_episode_return = -0.0053, mean_episode_step = 17.268, total_loss = 287.05, entropy_loss = -4.1125, pg_loss = 275.98, baseline_loss = 14.449, learner_queue_size = 32, _tick = 5, _time = 1.6985e+09)
Invalid runs, some did not end in a win or a loss:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
[[36m2023-10-28 13:01:56,214[39m][[34mroot[39m][[32mINFO[39m] - Step 17920 @ 511.2 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (train_seconds = 230.4, success_rate = 0.0, meta_entropy = tensor(1.3844), hks_loss = 0.28784, step = 17920, mean_episode_return = -0.0043423, mean_episode_step = 16.059, total_loss = 15.388, entropy_loss = -4.0967, pg_loss = 15.125, baseline_loss = 2.6007, learner_queue_size = 32, _tick = 6, _time = 1.6985e+09)
[[36m2023-10-28 13:02:01,226[39m][[34mroot[39m][[32mINFO[39m] - Step 17920 @ 0.0 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 235.4, success_rate = 0.0, meta_entropy = tensor(1.3844), hks_loss = 0.28784, step = 17920, mean_episode_return = -0.0043423, mean_episode_step = 16.059, total_loss = 15.388, entropy_loss = -4.0967, pg_loss = 15.125, baseline_loss = 2.6007, learner_queue_size = 32, _tick = 6, _time = 1.6985e+09)
[[36m2023-10-28 13:02:06,233[39m][[34mroot[39m][[32mINFO[39m] - Step 17920 @ 0.0 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 240.4, success_rate = 0.0, meta_entropy = tensor(1.3844), hks_loss = 0.28784, step = 17920, mean_episode_return = -0.0043423, mean_episode_step = 16.059, total_loss = 15.388, entropy_loss = -4.0967, pg_loss = 15.125, baseline_loss = 2.6007, learner_queue_size = 32, _tick = 6, _time = 1.6985e+09)
~ 20480 total_loss tensor(-66.9488, grad_fn=<AddBackward0>) ks_loss tensor(44.7934, grad_fn=<MulBackward0>)
### 20480 total_loss tensor(-22.1554, grad_fn=<AddBackward0>) hks_uniform_loss tensor(5.3789, grad_fn=<MulBackward0>)
[[36m2023-10-28 13:02:11,241[39m][[34mroot[39m][[32mINFO[39m] - Step 17920 @ 0.0 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (train_seconds = 245.4, success_rate = 0.0, meta_entropy = tensor(1.3844), hks_loss = 0.28784, step = 17920, mean_episode_return = -0.0043423, mean_episode_step = 16.059, total_loss = 15.388, entropy_loss = -4.0967, pg_loss = 15.125, baseline_loss = 2.6007, learner_queue_size = 32, _tick = 6, _time = 1.6985e+09)
Invalid runs, some did not end in a win or a loss:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.])
[[36m2023-10-28 13:02:16,249[39m][[34mroot[39m][[32mINFO[39m] - Step 20480 @ 511.2 SPS. Inference batcher size: 25. Learner queue size: 32. Other stats: (train_seconds = 250.5, success_rate = 0.0, meta_entropy = tensor(1.3842), hks_loss = 5.3789, step = 20480, mean_episode_return = -0.0057308, mean_episode_step = 19.479, total_loss = -16.777, entropy_loss = -3.661, pg_loss = -64.049, baseline_loss = 0.76071, learner_queue_size = 32, _tick = 7, _time = 1.6985e+09)
[[36m2023-10-28 13:02:21,258[39m][[34mroot[39m][[32mINFO[39m] - Step 20480 @ 0.0 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 255.5, success_rate = 0.0, meta_entropy = tensor(1.3842), hks_loss = 5.3789, step = 20480, mean_episode_return = -0.0057308, mean_episode_step = 19.479, total_loss = -16.777, entropy_loss = -3.661, pg_loss = -64.049, baseline_loss = 0.76071, learner_queue_size = 32, _tick = 7, _time = 1.6985e+09)
[[36m2023-10-28 13:02:26,265[39m][[34mroot[39m][[32mINFO[39m] - Step 20480 @ 0.0 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (train_seconds = 260.5, success_rate = 0.0, meta_entropy = tensor(1.3842), hks_loss = 5.3789, step = 20480, mean_episode_return = -0.0057308, mean_episode_step = 19.479, total_loss = -16.777, entropy_loss = -3.661, pg_loss = -64.049, baseline_loss = 0.76071, learner_queue_size = 32, _tick = 7, _time = 1.6985e+09)
~ 23040 total_loss tensor(38.1540, grad_fn=<AddBackward0>) ks_loss tensor(4.7291, grad_fn=<MulBackward0>)
### 23040 total_loss tensor(42.8831, grad_fn=<AddBackward0>) hks_uniform_loss tensor(1.3554, grad_fn=<MulBackward0>)
[[36m2023-10-28 13:02:31,274[39m][[34mroot[39m][[32mINFO[39m] - Step 20480 @ 0.0 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (train_seconds = 265.5, success_rate = 0.0, meta_entropy = tensor(1.3842), hks_loss = 5.3789, step = 20480, mean_episode_return = -0.0057308, mean_episode_step = 19.479, total_loss = -16.777, entropy_loss = -3.661, pg_loss = -64.049, baseline_loss = 0.76071, learner_queue_size = 32, _tick = 7, _time = 1.6985e+09)
Invalid runs, some did not end in a win or a loss:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0.])
[[36m2023-10-28 13:02:36,281[39m][[34mroot[39m][[32mINFO[39m] - Step 23040 @ 511.2 SPS. Inference batcher size: 90. Learner queue size: 32. Other stats: (train_seconds = 270.5, success_rate = 0.0, meta_entropy = tensor(1.3841), hks_loss = 1.3554, step = 23040, mean_episode_return = -0.004256, mean_episode_step = 15.033, total_loss = 44.239, entropy_loss = -4.0527, pg_loss = 39.942, baseline_loss = 2.2651, learner_queue_size = 32, _tick = 8, _time = 1.6985e+09)
[[36m2023-10-28 13:02:41,289[39m][[34mroot[39m][[32mINFO[39m] - Step 23040 @ 0.0 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (train_seconds = 275.5, success_rate = 0.0, meta_entropy = tensor(1.3841), hks_loss = 1.3554, step = 23040, mean_episode_return = -0.004256, mean_episode_step = 15.033, total_loss = 44.239, entropy_loss = -4.0527, pg_loss = 39.942, baseline_loss = 2.2651, learner_queue_size = 32, _tick = 8, _time = 1.6985e+09)
[[36m2023-10-28 13:02:46,299[39m][[34mroot[39m][[32mINFO[39m] - Step 23040 @ 0.0 SPS. Inference batcher size: 26. Learner queue size: 32. Other stats: (train_seconds = 280.5, success_rate = 0.0, meta_entropy = tensor(1.3841), hks_loss = 1.3554, step = 23040, mean_episode_return = -0.004256, mean_episode_step = 15.033, total_loss = 44.239, entropy_loss = -4.0527, pg_loss = 39.942, baseline_loss = 2.2651, learner_queue_size = 32, _tick = 8, _time = 1.6985e+09)
~ 25600 total_loss tensor(-186.7136, grad_fn=<AddBackward0>) ks_loss tensor(0.4564, grad_fn=<MulBackward0>)
### 25600 total_loss tensor(-186.2572, grad_fn=<AddBackward0>) hks_uniform_loss tensor(0.7117, grad_fn=<MulBackward0>)
[[36m2023-10-28 13:02:51,305[39m][[34mroot[39m][[32mINFO[39m] - Step 23040 @ 0.0 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (train_seconds = 285.5, success_rate = 0.0, meta_entropy = tensor(1.3841), hks_loss = 1.3554, step = 23040, mean_episode_return = -0.004256, mean_episode_step = 15.033, total_loss = 44.239, entropy_loss = -4.0527, pg_loss = 39.942, baseline_loss = 2.2651, learner_queue_size = 32, _tick = 8, _time = 1.6985e+09)
[[36m2023-10-28 13:02:56,318[39m][[34mroot[39m][[32mINFO[39m] - Step 23040 @ 0.0 SPS. Inference batcher size: 58. Learner queue size: 32. Other stats: (train_seconds = 290.5, success_rate = 0.0, meta_entropy = tensor(1.3841), hks_loss = 1.3554, step = 23040, mean_episode_return = -0.004256, mean_episode_step = 15.033, total_loss = 44.239, entropy_loss = -4.0527, pg_loss = 39.942, baseline_loss = 2.2651, learner_queue_size = 32, _tick = 8, _time = 1.6985e+09)
Invalid runs, some did not end in a win or a loss:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
[[36m2023-10-28 13:03:01,323[39m][[34mroot[39m][[32mINFO[39m] - Saving checkpoint to /workspace/outputs/2023-10-28/12-58-00/quest_easy_0.25.tar
[[36m2023-10-28 13:03:01,355[39m][[34mroot[39m][[32mINFO[39m] - Step 25600 @ 510.8 SPS. Inference batcher size: 50. Learner queue size: 32. Other stats: (train_seconds = 295.5, success_rate = 0.0, meta_entropy = tensor(1.3843), hks_loss = 0.7117, step = 25600, mean_episode_return = -0.004693, mean_episode_step = 15.568, total_loss = -185.55, entropy_loss = -4.1076, pg_loss = -185.45, baseline_loss = 2.8472, learner_queue_size = 32, _tick = 9, _time = 1.6985e+09)
[[36m2023-10-28 13:03:06,362[39m][[34mroot[39m][[32mINFO[39m] - Step 25600 @ 0.0 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (train_seconds = 300.6, success_rate = 0.0, meta_entropy = tensor(1.3843), hks_loss = 0.7117, step = 25600, mean_episode_return = -0.004693, mean_episode_step = 15.568, total_loss = -185.55, entropy_loss = -4.1076, pg_loss = -185.45, baseline_loss = 2.8472, learner_queue_size = 32, _tick = 9, _time = 1.6985e+09)
[[36m2023-10-28 13:03:11,370[39m][[34mroot[39m][[32mINFO[39m] - Step 25600 @ 0.0 SPS. Inference batcher size: 45. Learner queue size: 32. Other stats: (train_seconds = 305.6, success_rate = 0.0, meta_entropy = tensor(1.3843), hks_loss = 0.7117, step = 25600, mean_episode_return = -0.004693, mean_episode_step = 15.568, total_loss = -185.55, entropy_loss = -4.1076, pg_loss = -185.45, baseline_loss = 2.8472, learner_queue_size = 32, _tick = 9, _time = 1.6985e+09)
~ 28160 total_loss tensor(-40.6128, grad_fn=<AddBackward0>) ks_loss tensor(0.3347, grad_fn=<MulBackward0>)
### 28160 total_loss tensor(-40.2780, grad_fn=<AddBackward0>) hks_uniform_loss tensor(0.0684, grad_fn=<MulBackward0>)
[[36m2023-10-28 13:03:16,379[39m][[34mroot[39m][[32mINFO[39m] - Step 25600 @ 0.0 SPS. Inference batcher size: 74. Learner queue size: 32. Other stats: (train_seconds = 310.6, success_rate = 0.0, meta_entropy = tensor(1.3843), hks_loss = 0.7117, step = 25600, mean_episode_return = -0.004693, mean_episode_step = 15.568, total_loss = -185.55, entropy_loss = -4.1076, pg_loss = -185.45, baseline_loss = 2.8472, learner_queue_size = 32, _tick = 9, _time = 1.6985e+09)
Invalid runs, some did not end in a win or a loss:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0.])
[[36m2023-10-28 13:03:21,385[39m][[34mroot[39m][[32mINFO[39m] - Step 28160 @ 511.4 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 315.6, success_rate = 0.0, meta_entropy = tensor(1.3838), hks_loss = 0.068399, step = 28160, mean_episode_return = -0.004561, mean_episode_step = 16.692, total_loss = -40.21, entropy_loss = -4.1154, pg_loss = -41.062, baseline_loss = 4.5642, learner_queue_size = 32, _tick = 10, _time = 1.6985e+09)
[[36m2023-10-28 13:03:26,394[39m][[34mroot[39m][[32mINFO[39m] - Step 28160 @ 0.0 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (train_seconds = 320.6, success_rate = 0.0, meta_entropy = tensor(1.3838), hks_loss = 0.068399, step = 28160, mean_episode_return = -0.004561, mean_episode_step = 16.692, total_loss = -40.21, entropy_loss = -4.1154, pg_loss = -41.062, baseline_loss = 4.5642, learner_queue_size = 32, _tick = 10, _time = 1.6985e+09)
[[36m2023-10-28 13:03:31,400[39m][[34mroot[39m][[32mINFO[39m] - Step 28160 @ 0.0 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 325.6, success_rate = 0.0, meta_entropy = tensor(1.3838), hks_loss = 0.068399, step = 28160, mean_episode_return = -0.004561, mean_episode_step = 16.692, total_loss = -40.21, entropy_loss = -4.1154, pg_loss = -41.062, baseline_loss = 4.5642, learner_queue_size = 32, _tick = 10, _time = 1.6985e+09)
~ 30720 total_loss tensor(122.9311, grad_fn=<AddBackward0>) ks_loss tensor(2.4599, grad_fn=<MulBackward0>)
### 30720 total_loss tensor(125.3910, grad_fn=<AddBackward0>) hks_uniform_loss tensor(0.1375, grad_fn=<MulBackward0>)
[[36m2023-10-28 13:03:36,438[39m][[34mroot[39m][[32mINFO[39m] - Step 28160 @ 0.0 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (train_seconds = 330.6, success_rate = 0.0, meta_entropy = tensor(1.3838), hks_loss = 0.068399, step = 28160, mean_episode_return = -0.004561, mean_episode_step = 16.692, total_loss = -40.21, entropy_loss = -4.1154, pg_loss = -41.062, baseline_loss = 4.5642, learner_queue_size = 32, _tick = 10, _time = 1.6985e+09)
Invalid runs, some did not end in a win or a loss:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
[[36m2023-10-28 13:03:41,445[39m][[34mroot[39m][[32mINFO[39m] - Step 30720 @ 507.9 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (train_seconds = 335.7, success_rate = 0.0, meta_entropy = tensor(1.3838), hks_loss = 0.13748, step = 30720, mean_episode_return = -0.0048083, mean_episode_step = 18.183, total_loss = 125.53, entropy_loss = -4.0769, pg_loss = 121.78, baseline_loss = 5.2272, learner_queue_size = 32, _tick = 11, _time = 1.6985e+09)
[[36m2023-10-28 13:03:46,454[39m][[34mroot[39m][[32mINFO[39m] - Step 30720 @ 0.0 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (train_seconds = 340.7, success_rate = 0.0, meta_entropy = tensor(1.3838), hks_loss = 0.13748, step = 30720, mean_episode_return = -0.0048083, mean_episode_step = 18.183, total_loss = 125.53, entropy_loss = -4.0769, pg_loss = 121.78, baseline_loss = 5.2272, learner_queue_size = 32, _tick = 11, _time = 1.6985e+09)
[[36m2023-10-28 13:03:51,462[39m][[34mroot[39m][[32mINFO[39m] - Step 30720 @ 0.0 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (train_seconds = 345.7, success_rate = 0.0, meta_entropy = tensor(1.3838), hks_loss = 0.13748, step = 30720, mean_episode_return = -0.0048083, mean_episode_step = 18.183, total_loss = 125.53, entropy_loss = -4.0769, pg_loss = 121.78, baseline_loss = 5.2272, learner_queue_size = 32, _tick = 11, _time = 1.6985e+09)
~ 33280 total_loss tensor(-138.5150, grad_fn=<AddBackward0>) ks_loss tensor(1.4450, grad_fn=<MulBackward0>)
### 33280 total_loss tensor(-137.0700, grad_fn=<AddBackward0>) hks_uniform_loss tensor(0.0744, grad_fn=<MulBackward0>)
[[36m2023-10-28 13:03:56,470[39m][[34mroot[39m][[32mINFO[39m] - Step 30720 @ 0.0 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (train_seconds = 350.7, success_rate = 0.0, meta_entropy = tensor(1.3838), hks_loss = 0.13748, step = 30720, mean_episode_return = -0.0048083, mean_episode_step = 18.183, total_loss = 125.53, entropy_loss = -4.0769, pg_loss = 121.78, baseline_loss = 5.2272, learner_queue_size = 32, _tick = 11, _time = 1.6985e+09)
Invalid runs, some did not end in a win or a loss:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0.])
[[36m2023-10-28 13:04:01,482[39m][[34mroot[39m][[32mINFO[39m] - Step 33280 @ 510.8 SPS. Inference batcher size: 36. Learner queue size: 32. Other stats: (train_seconds = 355.7, success_rate = 0.0, meta_entropy = tensor(1.3835), hks_loss = 0.074366, step = 33280, mean_episode_return = -0.0044574, mean_episode_step = 16.926, total_loss = -137.0, entropy_loss = -4.0918, pg_loss = -136.48, baseline_loss = 2.06, learner_queue_size = 32, _tick = 12, _time = 1.6985e+09)
[[36m2023-10-28 13:04:06,489[39m][[34mroot[39m][[32mINFO[39m] - Step 33280 @ 0.0 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (train_seconds = 360.7, success_rate = 0.0, meta_entropy = tensor(1.3835), hks_loss = 0.074366, step = 33280, mean_episode_return = -0.0044574, mean_episode_step = 16.926, total_loss = -137.0, entropy_loss = -4.0918, pg_loss = -136.48, baseline_loss = 2.06, learner_queue_size = 32, _tick = 12, _time = 1.6985e+09)
[[36m2023-10-28 13:04:11,495[39m][[34mroot[39m][[32mINFO[39m] - Step 33280 @ 0.0 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 365.7, success_rate = 0.0, meta_entropy = tensor(1.3835), hks_loss = 0.074366, step = 33280, mean_episode_return = -0.0044574, mean_episode_step = 16.926, total_loss = -137.0, entropy_loss = -4.0918, pg_loss = -136.48, baseline_loss = 2.06, learner_queue_size = 32, _tick = 12, _time = 1.6985e+09)
~ 35840 total_loss tensor(55.8655, grad_fn=<AddBackward0>) ks_loss tensor(0.9127, grad_fn=<MulBackward0>)
### 35840 total_loss tensor(56.7782, grad_fn=<AddBackward0>) hks_uniform_loss tensor(0.0162, grad_fn=<MulBackward0>)
[[36m2023-10-28 13:04:16,502[39m][[34mroot[39m][[32mINFO[39m] - Step 33280 @ 0.0 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (train_seconds = 370.7, success_rate = 0.0, meta_entropy = tensor(1.3835), hks_loss = 0.074366, step = 33280, mean_episode_return = -0.0044574, mean_episode_step = 16.926, total_loss = -137.0, entropy_loss = -4.0918, pg_loss = -136.48, baseline_loss = 2.06, learner_queue_size = 32, _tick = 12, _time = 1.6985e+09)
Invalid runs, some did not end in a win or a loss:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.])
[[36m2023-10-28 13:04:21,509[39m][[34mroot[39m][[32mINFO[39m] - Step 35840 @ 511.2 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 375.7, success_rate = 0.0, meta_entropy = tensor(1.3839), hks_loss = 0.016245, step = 35840, mean_episode_return = -0.0035616, mean_episode_step = 15.831, total_loss = 56.794, entropy_loss = -4.0994, pg_loss = 55.743, baseline_loss = 4.2223, learner_queue_size = 32, _tick = 13, _time = 1.6985e+09)
[[36m2023-10-28 13:04:26,519[39m][[34mroot[39m][[32mINFO[39m] - Step 35840 @ 0.0 SPS. Inference batcher size: 23. Learner queue size: 32. Other stats: (train_seconds = 380.7, success_rate = 0.0, meta_entropy = tensor(1.3839), hks_loss = 0.016245, step = 35840, mean_episode_return = -0.0035616, mean_episode_step = 15.831, total_loss = 56.794, entropy_loss = -4.0994, pg_loss = 55.743, baseline_loss = 4.2223, learner_queue_size = 32, _tick = 13, _time = 1.6985e+09)
[[36m2023-10-28 13:04:31,530[39m][[34mroot[39m][[32mINFO[39m] - Step 35840 @ 0.0 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (train_seconds = 385.7, success_rate = 0.0, meta_entropy = tensor(1.3839), hks_loss = 0.016245, step = 35840, mean_episode_return = -0.0035616, mean_episode_step = 15.831, total_loss = 56.794, entropy_loss = -4.0994, pg_loss = 55.743, baseline_loss = 4.2223, learner_queue_size = 32, _tick = 13, _time = 1.6985e+09)
~ 38400 total_loss tensor(-1.3619, grad_fn=<AddBackward0>) ks_loss tensor(3.8146, grad_fn=<MulBackward0>)
### 38400 total_loss tensor(2.4527, grad_fn=<AddBackward0>) hks_uniform_loss tensor(0.0212, grad_fn=<MulBackward0>)
[[36m2023-10-28 13:04:36,538[39m][[34mroot[39m][[32mINFO[39m] - Step 35840 @ 0.0 SPS. Inference batcher size: 22. Learner queue size: 32. Other stats: (train_seconds = 390.7, success_rate = 0.0, meta_entropy = tensor(1.3839), hks_loss = 0.016245, step = 35840, mean_episode_return = -0.0035616, mean_episode_step = 15.831, total_loss = 56.794, entropy_loss = -4.0994, pg_loss = 55.743, baseline_loss = 4.2223, learner_queue_size = 32, _tick = 13, _time = 1.6985e+09)
Invalid runs, some did not end in a win or a loss:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
[[36m2023-10-28 13:04:41,550[39m][[34mroot[39m][[32mINFO[39m] - Step 38400 @ 510.9 SPS. Inference batcher size: 45. Learner queue size: 32. Other stats: (train_seconds = 395.8, success_rate = 0.0, meta_entropy = tensor(1.3839), hks_loss = 0.021212, step = 38400, mean_episode_return = -0.0032238, mean_episode_step = 13.154, total_loss = 2.4739, entropy_loss = -4.0599, pg_loss = 1.3354, baseline_loss = 1.3627, learner_queue_size = 32, _tick = 14, _time = 1.6985e+09)
[[36m2023-10-28 13:04:46,556[39m][[34mroot[39m][[32mINFO[39m] - Step 38400 @ 0.0 SPS. Inference batcher size: 35. Learner queue size: 32. Other stats: (train_seconds = 400.8, success_rate = 0.0, meta_entropy = tensor(1.3839), hks_loss = 0.021212, step = 38400, mean_episode_return = -0.0032238, mean_episode_step = 13.154, total_loss = 2.4739, entropy_loss = -4.0599, pg_loss = 1.3354, baseline_loss = 1.3627, learner_queue_size = 32, _tick = 14, _time = 1.6985e+09)
[[36m2023-10-28 13:04:51,562[39m][[34mroot[39m][[32mINFO[39m] - Step 38400 @ 0.0 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (train_seconds = 405.8, success_rate = 0.0, meta_entropy = tensor(1.3839), hks_loss = 0.021212, step = 38400, mean_episode_return = -0.0032238, mean_episode_step = 13.154, total_loss = 2.4739, entropy_loss = -4.0599, pg_loss = 1.3354, baseline_loss = 1.3627, learner_queue_size = 32, _tick = 14, _time = 1.6985e+09)
~ 40960 total_loss tensor(-53.1597, grad_fn=<AddBackward0>) ks_loss tensor(1.1232, grad_fn=<MulBackward0>)
### 40960 total_loss tensor(-52.0365, grad_fn=<AddBackward0>) hks_uniform_loss tensor(0.0424, grad_fn=<MulBackward0>)
[[36m2023-10-28 13:04:56,570[39m][[34mroot[39m][[32mINFO[39m] - Step 38400 @ 0.0 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (train_seconds = 410.8, success_rate = 0.0, meta_entropy = tensor(1.3839), hks_loss = 0.021212, step = 38400, mean_episode_return = -0.0032238, mean_episode_step = 13.154, total_loss = 2.4739, entropy_loss = -4.0599, pg_loss = 1.3354, baseline_loss = 1.3627, learner_queue_size = 32, _tick = 14, _time = 1.6985e+09)
Invalid runs, some did not end in a win or a loss:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
[[36m2023-10-28 13:05:01,576[39m][[34mroot[39m][[32mINFO[39m] - Step 40960 @ 511.2 SPS. Inference batcher size: 82. Learner queue size: 32. Other stats: (train_seconds = 415.8, success_rate = 0.0, meta_entropy = tensor(1.3845), hks_loss = 0.042387, step = 40960, mean_episode_return = -0.0035878, mean_episode_step = 14.586, total_loss = -51.994, entropy_loss = -4.0943, pg_loss = -51.565, baseline_loss = 2.4999, learner_queue_size = 32, _tick = 15, _time = 1.6985e+09)
[[36m2023-10-28 13:05:06,581[39m][[34mroot[39m][[32mINFO[39m] - Step 40960 @ 0.0 SPS. Inference batcher size: 52. Learner queue size: 32. Other stats: (train_seconds = 420.8, success_rate = 0.0, meta_entropy = tensor(1.3845), hks_loss = 0.042387, step = 40960, mean_episode_return = -0.0035878, mean_episode_step = 14.586, total_loss = -51.994, entropy_loss = -4.0943, pg_loss = -51.565, baseline_loss = 2.4999, learner_queue_size = 32, _tick = 15, _time = 1.6985e+09)
[[36m2023-10-28 13:05:11,592[39m][[34mroot[39m][[32mINFO[39m] - Step 40960 @ 0.0 SPS. Inference batcher size: 37. Learner queue size: 32. Other stats: (train_seconds = 425.8, success_rate = 0.0, meta_entropy = tensor(1.3845), hks_loss = 0.042387, step = 40960, mean_episode_return = -0.0035878, mean_episode_step = 14.586, total_loss = -51.994, entropy_loss = -4.0943, pg_loss = -51.565, baseline_loss = 2.4999, learner_queue_size = 32, _tick = 15, _time = 1.6985e+09)
[[36m2023-10-28 13:05:16,596[39m][[34mroot[39m][[32mINFO[39m] - Step 40960 @ 0.0 SPS. Inference batcher size: 45. Learner queue size: 32. Other stats: (train_seconds = 430.8, success_rate = 0.0, meta_entropy = tensor(1.3845), hks_loss = 0.042387, step = 40960, mean_episode_return = -0.0035878, mean_episode_step = 14.586, total_loss = -51.994, entropy_loss = -4.0943, pg_loss = -51.565, baseline_loss = 2.4999, learner_queue_size = 32, _tick = 15, _time = 1.6985e+09)
~ 43520 total_loss tensor(22.2488, grad_fn=<AddBackward0>) ks_loss tensor(1.9420, grad_fn=<MulBackward0>)
### 43520 total_loss tensor(24.1909, grad_fn=<AddBackward0>) hks_uniform_loss tensor(0.0238, grad_fn=<MulBackward0>)
[[36m2023-10-28 13:05:21,601[39m][[34mroot[39m][[32mINFO[39m] - Step 40960 @ 0.0 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 435.8, success_rate = 0.0, meta_entropy = tensor(1.3845), hks_loss = 0.042387, step = 40960, mean_episode_return = -0.0035878, mean_episode_step = 14.586, total_loss = -51.994, entropy_loss = -4.0943, pg_loss = -51.565, baseline_loss = 2.4999, learner_queue_size = 32, _tick = 15, _time = 1.6985e+09)
Invalid runs, some did not end in a win or a loss:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
[[36m2023-10-28 13:05:26,607[39m][[34mroot[39m][[32mINFO[39m] - Step 43520 @ 511.4 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (train_seconds = 440.8, success_rate = 0.0, meta_entropy = tensor(1.3849), hks_loss = 0.023808, step = 43520, mean_episode_return = -0.003735, mean_episode_step = 15.672, total_loss = 24.215, entropy_loss = -4.0811, pg_loss = 23.768, baseline_loss = 2.5624, learner_queue_size = 32, _tick = 16, _time = 1.6985e+09)
[[36m2023-10-28 13:05:31,617[39m][[34mroot[39m][[32mINFO[39m] - Step 43520 @ 0.0 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 445.8, success_rate = 0.0, meta_entropy = tensor(1.3849), hks_loss = 0.023808, step = 43520, mean_episode_return = -0.003735, mean_episode_step = 15.672, total_loss = 24.215, entropy_loss = -4.0811, pg_loss = 23.768, baseline_loss = 2.5624, learner_queue_size = 32, _tick = 16, _time = 1.6985e+09)
[[36m2023-10-28 13:05:36,625[39m][[34mroot[39m][[32mINFO[39m] - Step 43520 @ 0.0 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (train_seconds = 450.8, success_rate = 0.0, meta_entropy = tensor(1.3849), hks_loss = 0.023808, step = 43520, mean_episode_return = -0.003735, mean_episode_step = 15.672, total_loss = 24.215, entropy_loss = -4.0811, pg_loss = 23.768, baseline_loss = 2.5624, learner_queue_size = 32, _tick = 16, _time = 1.6985e+09)
~ 46080 total_loss tensor(-56.7329, grad_fn=<AddBackward0>) ks_loss tensor(1.8826, grad_fn=<MulBackward0>)
### 46080 total_loss tensor(-54.8503, grad_fn=<AddBackward0>) hks_uniform_loss tensor(0.0097, grad_fn=<MulBackward0>)
[[36m2023-10-28 13:05:41,634[39m][[34mroot[39m][[32mINFO[39m] - Step 43520 @ 0.0 SPS. Inference batcher size: 22. Learner queue size: 32. Other stats: (train_seconds = 455.8, success_rate = 0.0, meta_entropy = tensor(1.3849), hks_loss = 0.023808, step = 43520, mean_episode_return = -0.003735, mean_episode_step = 15.672, total_loss = 24.215, entropy_loss = -4.0811, pg_loss = 23.768, baseline_loss = 2.5624, learner_queue_size = 32, _tick = 16, _time = 1.6985e+09)
Invalid runs, some did not end in a win or a loss:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
[[36m2023-10-28 13:05:46,641[39m][[34mroot[39m][[32mINFO[39m] - Step 46080 @ 511.4 SPS. Inference batcher size: 41. Learner queue size: 32. Other stats: (train_seconds = 460.8, success_rate = 0.0, meta_entropy = tensor(1.3855), hks_loss = 0.0097244, step = 46080, mean_episode_return = -0.0038978, mean_episode_step = 15.306, total_loss = -54.841, entropy_loss = -4.0811, pg_loss = -55.558, baseline_loss = 2.9064, learner_queue_size = 32, _tick = 17, _time = 1.6985e+09)
[[36m2023-10-28 13:05:51,650[39m][[34mroot[39m][[32mINFO[39m] - Step 46080 @ 0.0 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (train_seconds = 465.9, success_rate = 0.0, meta_entropy = tensor(1.3855), hks_loss = 0.0097244, step = 46080, mean_episode_return = -0.0038978, mean_episode_step = 15.306, total_loss = -54.841, entropy_loss = -4.0811, pg_loss = -55.558, baseline_loss = 2.9064, learner_queue_size = 32, _tick = 17, _time = 1.6985e+09)
[[36m2023-10-28 13:05:56,656[39m][[34mroot[39m][[32mINFO[39m] - Step 46080 @ 0.0 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (train_seconds = 470.9, success_rate = 0.0, meta_entropy = tensor(1.3855), hks_loss = 0.0097244, step = 46080, mean_episode_return = -0.0038978, mean_episode_step = 15.306, total_loss = -54.841, entropy_loss = -4.0811, pg_loss = -55.558, baseline_loss = 2.9064, learner_queue_size = 32, _tick = 17, _time = 1.6985e+09)
~ 48640 total_loss tensor(35.1657, grad_fn=<AddBackward0>) ks_loss tensor(2.5882, grad_fn=<MulBackward0>)
### 48640 total_loss tensor(37.7539, grad_fn=<AddBackward0>) hks_uniform_loss tensor(0.0294, grad_fn=<MulBackward0>)
[[36m2023-10-28 13:06:01,662[39m][[34mroot[39m][[32mINFO[39m] - Step 46080 @ 0.0 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (train_seconds = 475.9, success_rate = 0.0, meta_entropy = tensor(1.3855), hks_loss = 0.0097244, step = 46080, mean_episode_return = -0.0038978, mean_episode_step = 15.306, total_loss = -54.841, entropy_loss = -4.0811, pg_loss = -55.558, baseline_loss = 2.9064, learner_queue_size = 32, _tick = 17, _time = 1.6985e+09)
Invalid runs, some did not end in a win or a loss:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0.])
[[36m2023-10-28 13:06:06,673[39m][[34mroot[39m][[32mINFO[39m] - Step 48640 @ 510.8 SPS. Inference batcher size: 39. Learner queue size: 32. Other stats: (train_seconds = 480.9, success_rate = 0.0, meta_entropy = tensor(1.3858), hks_loss = 0.029438, step = 48640, mean_episode_return = -0.003626, mean_episode_step = 17.105, total_loss = 37.783, entropy_loss = -4.0734, pg_loss = 35.606, baseline_loss = 3.6332, learner_queue_size = 32, _tick = 18, _time = 1.6985e+09)
[[36m2023-10-28 13:06:11,681[39m][[34mroot[39m][[32mINFO[39m] - Step 48640 @ 0.0 SPS. Inference batcher size: 66. Learner queue size: 32. Other stats: (train_seconds = 485.9, success_rate = 0.0, meta_entropy = tensor(1.3858), hks_loss = 0.029438, step = 48640, mean_episode_return = -0.003626, mean_episode_step = 17.105, total_loss = 37.783, entropy_loss = -4.0734, pg_loss = 35.606, baseline_loss = 3.6332, learner_queue_size = 32, _tick = 18, _time = 1.6985e+09)
[[36m2023-10-28 13:06:16,708[39m][[34mroot[39m][[32mINFO[39m] - Step 48640 @ 0.0 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 490.9, success_rate = 0.0, meta_entropy = tensor(1.3858), hks_loss = 0.029438, step = 48640, mean_episode_return = -0.003626, mean_episode_step = 17.105, total_loss = 37.783, entropy_loss = -4.0734, pg_loss = 35.606, baseline_loss = 3.6332, learner_queue_size = 32, _tick = 18, _time = 1.6985e+09)
~ 51200 total_loss tensor(-28.2499, grad_fn=<AddBackward0>) ks_loss tensor(3.9841, grad_fn=<MulBackward0>)
### 51200 total_loss tensor(-24.2658, grad_fn=<AddBackward0>) hks_uniform_loss tensor(0.0176, grad_fn=<MulBackward0>)
[[36m2023-10-28 13:06:21,713[39m][[34mroot[39m][[32mINFO[39m] - Step 48640 @ 0.0 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (train_seconds = 495.9, success_rate = 0.0, meta_entropy = tensor(1.3858), hks_loss = 0.029438, step = 48640, mean_episode_return = -0.003626, mean_episode_step = 17.105, total_loss = 37.783, entropy_loss = -4.0734, pg_loss = 35.606, baseline_loss = 3.6332, learner_queue_size = 32, _tick = 18, _time = 1.6985e+09)
Invalid runs, some did not end in a win or a loss:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
[[36m2023-10-28 13:06:26,720[39m][[34mroot[39m][[32mINFO[39m] - Saving checkpoint to /workspace/outputs/2023-10-28/12-58-00/quest_easy_0.5.tar
[[36m2023-10-28 13:06:26,819[39m][[34mroot[39m][[32mINFO[39m] - Step 51200 @ 511.2 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 500.9, success_rate = 0.0, meta_entropy = tensor(1.3857), hks_loss = 0.017563, step = 51200, mean_episode_return = -0.0029078, mean_episode_step = 12.346, total_loss = -24.248, entropy_loss = -4.0566, pg_loss = -25.774, baseline_loss = 1.5802, learner_queue_size = 32, _tick = 19, _time = 1.6985e+09)
[[36m2023-10-28 13:06:31,827[39m][[34mroot[39m][[32mINFO[39m] - Step 51200 @ 0.0 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (train_seconds = 506.0, success_rate = 0.0, meta_entropy = tensor(1.3857), hks_loss = 0.017563, step = 51200, mean_episode_return = -0.0029078, mean_episode_step = 12.346, total_loss = -24.248, entropy_loss = -4.0566, pg_loss = -25.774, baseline_loss = 1.5802, learner_queue_size = 32, _tick = 19, _time = 1.6985e+09)
[[36m2023-10-28 13:06:36,836[39m][[34mroot[39m][[32mINFO[39m] - Step 51200 @ 0.0 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (train_seconds = 511.0, success_rate = 0.0, meta_entropy = tensor(1.3857), hks_loss = 0.017563, step = 51200, mean_episode_return = -0.0029078, mean_episode_step = 12.346, total_loss = -24.248, entropy_loss = -4.0566, pg_loss = -25.774, baseline_loss = 1.5802, learner_queue_size = 32, _tick = 19, _time = 1.6985e+09)
~ 53760 total_loss tensor(1.1403, grad_fn=<AddBackward0>) ks_loss tensor(1.5040, grad_fn=<MulBackward0>)
### 53760 total_loss tensor(2.6443, grad_fn=<AddBackward0>) hks_uniform_loss tensor(0.0179, grad_fn=<MulBackward0>)
[[36m2023-10-28 13:06:41,847[39m][[34mroot[39m][[32mINFO[39m] - Step 51200 @ 0.0 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (train_seconds = 516.0, success_rate = 0.0, meta_entropy = tensor(1.3857), hks_loss = 0.017563, step = 51200, mean_episode_return = -0.0029078, mean_episode_step = 12.346, total_loss = -24.248, entropy_loss = -4.0566, pg_loss = -25.774, baseline_loss = 1.5802, learner_queue_size = 32, _tick = 19, _time = 1.6985e+09)
Invalid runs, some did not end in a win or a loss:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.])
[[36m2023-10-28 13:06:46,853[39m][[34mroot[39m][[32mINFO[39m] - Step 53760 @ 510.9 SPS. Inference batcher size: 70. Learner queue size: 32. Other stats: (train_seconds = 521.1, success_rate = 0.0, meta_entropy = tensor(1.3857), hks_loss = 0.017919, step = 53760, mean_episode_return = -0.0035517, mean_episode_step = 14.437, total_loss = 2.6622, entropy_loss = -4.0884, pg_loss = 2.516, baseline_loss = 2.7127, learner_queue_size = 32, _tick = 20, _time = 1.6985e+09)
[[36m2023-10-28 13:06:51,858[39m][[34mroot[39m][[32mINFO[39m] - Step 53760 @ 0.0 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 526.1, success_rate = 0.0, meta_entropy = tensor(1.3857), hks_loss = 0.017919, step = 53760, mean_episode_return = -0.0035517, mean_episode_step = 14.437, total_loss = 2.6622, entropy_loss = -4.0884, pg_loss = 2.516, baseline_loss = 2.7127, learner_queue_size = 32, _tick = 20, _time = 1.6985e+09)
[[36m2023-10-28 13:06:56,871[39m][[34mroot[39m][[32mINFO[39m] - Step 53760 @ 0.0 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (train_seconds = 531.1, success_rate = 0.0, meta_entropy = tensor(1.3857), hks_loss = 0.017919, step = 53760, mean_episode_return = -0.0035517, mean_episode_step = 14.437, total_loss = 2.6622, entropy_loss = -4.0884, pg_loss = 2.516, baseline_loss = 2.7127, learner_queue_size = 32, _tick = 20, _time = 1.6985e+09)
[[36m2023-10-28 13:07:01,885[39m][[34mroot[39m][[32mINFO[39m] - Step 53760 @ 0.0 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (train_seconds = 536.1, success_rate = 0.0, meta_entropy = tensor(1.3857), hks_loss = 0.017919, step = 53760, mean_episode_return = -0.0035517, mean_episode_step = 14.437, total_loss = 2.6622, entropy_loss = -4.0884, pg_loss = 2.516, baseline_loss = 2.7127, learner_queue_size = 32, _tick = 20, _time = 1.6985e+09)
~ 56320 total_loss tensor(15.7624, grad_fn=<AddBackward0>) ks_loss tensor(3.2944, grad_fn=<MulBackward0>)
### 56320 total_loss tensor(19.0569, grad_fn=<AddBackward0>) hks_uniform_loss tensor(0.0317, grad_fn=<MulBackward0>)
[[36m2023-10-28 13:07:06,889[39m][[34mroot[39m][[32mINFO[39m] - Step 53760 @ 0.0 SPS. Inference batcher size: 45. Learner queue size: 32. Other stats: (train_seconds = 541.1, success_rate = 0.0, meta_entropy = tensor(1.3857), hks_loss = 0.017919, step = 53760, mean_episode_return = -0.0035517, mean_episode_step = 14.437, total_loss = 2.6622, entropy_loss = -4.0884, pg_loss = 2.516, baseline_loss = 2.7127, learner_queue_size = 32, _tick = 20, _time = 1.6985e+09)
Invalid runs, some did not end in a win or a loss:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
[[36m2023-10-28 13:07:11,898[39m][[34mroot[39m][[32mINFO[39m] - Step 56320 @ 511.2 SPS. Inference batcher size: 41. Learner queue size: 32. Other stats: (train_seconds = 546.1, success_rate = 0.0, meta_entropy = tensor(1.3861), hks_loss = 0.031681, step = 56320, mean_episode_return = -0.0030435, mean_episode_step = 13.804, total_loss = 19.089, entropy_loss = -4.0633, pg_loss = 17.673, baseline_loss = 2.153, learner_queue_size = 32, _tick = 21, _time = 1.6985e+09)
[[36m2023-10-28 13:07:16,914[39m][[34mroot[39m][[32mINFO[39m] - Step 56320 @ 0.0 SPS. Inference batcher size: 52. Learner queue size: 32. Other stats: (train_seconds = 551.1, success_rate = 0.0, meta_entropy = tensor(1.3861), hks_loss = 0.031681, step = 56320, mean_episode_return = -0.0030435, mean_episode_step = 13.804, total_loss = 19.089, entropy_loss = -4.0633, pg_loss = 17.673, baseline_loss = 2.153, learner_queue_size = 32, _tick = 21, _time = 1.6985e+09)
[[36m2023-10-28 13:07:21,924[39m][[34mroot[39m][[32mINFO[39m] - Step 56320 @ 0.0 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (train_seconds = 556.1, success_rate = 0.0, meta_entropy = tensor(1.3861), hks_loss = 0.031681, step = 56320, mean_episode_return = -0.0030435, mean_episode_step = 13.804, total_loss = 19.089, entropy_loss = -4.0633, pg_loss = 17.673, baseline_loss = 2.153, learner_queue_size = 32, _tick = 21, _time = 1.6985e+09)
~ 58880 total_loss tensor(-113.7871, grad_fn=<AddBackward0>) ks_loss tensor(2.1411, grad_fn=<MulBackward0>)
### 58880 total_loss tensor(-111.6460, grad_fn=<AddBackward0>) hks_uniform_loss tensor(0.0174, grad_fn=<MulBackward0>)
[[36m2023-10-28 13:07:26,930[39m][[34mroot[39m][[32mINFO[39m] - Step 56320 @ 0.0 SPS. Inference batcher size: 92. Learner queue size: 32. Other stats: (train_seconds = 561.1, success_rate = 0.0, meta_entropy = tensor(1.3861), hks_loss = 0.031681, step = 56320, mean_episode_return = -0.0030435, mean_episode_step = 13.804, total_loss = 19.089, entropy_loss = -4.0633, pg_loss = 17.673, baseline_loss = 2.153, learner_queue_size = 32, _tick = 21, _time = 1.6985e+09)
Invalid runs, some did not end in a win or a loss:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.])
[[36m2023-10-28 13:07:31,939[39m][[34mroot[39m][[32mINFO[39m] - Step 58880 @ 511.2 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (train_seconds = 566.1, success_rate = 0.0, meta_entropy = tensor(1.3862), hks_loss = 0.017434, step = 58880, mean_episode_return = -0.0034274, mean_episode_step = 15.11, total_loss = -111.63, entropy_loss = -4.0794, pg_loss = -113.98, baseline_loss = 4.2765, learner_queue_size = 32, _tick = 22, _time = 1.6985e+09)
[[36m2023-10-28 13:07:36,949[39m][[34mroot[39m][[32mINFO[39m] - Step 58880 @ 0.0 SPS. Inference batcher size: 40. Learner queue size: 32. Other stats: (train_seconds = 571.2, success_rate = 0.0, meta_entropy = tensor(1.3862), hks_loss = 0.017434, step = 58880, mean_episode_return = -0.0034274, mean_episode_step = 15.11, total_loss = -111.63, entropy_loss = -4.0794, pg_loss = -113.98, baseline_loss = 4.2765, learner_queue_size = 32, _tick = 22, _time = 1.6985e+09)
[[36m2023-10-28 13:07:41,957[39m][[34mroot[39m][[32mINFO[39m] - Step 58880 @ 0.0 SPS. Inference batcher size: 37. Learner queue size: 32. Other stats: (train_seconds = 576.2, success_rate = 0.0, meta_entropy = tensor(1.3862), hks_loss = 0.017434, step = 58880, mean_episode_return = -0.0034274, mean_episode_step = 15.11, total_loss = -111.63, entropy_loss = -4.0794, pg_loss = -113.98, baseline_loss = 4.2765, learner_queue_size = 32, _tick = 22, _time = 1.6985e+09)
~ 61440 total_loss tensor(81.9761, grad_fn=<AddBackward0>) ks_loss tensor(2.7620, grad_fn=<MulBackward0>)
### 61440 total_loss tensor(84.7380, grad_fn=<AddBackward0>) hks_uniform_loss tensor(0.0261, grad_fn=<MulBackward0>)
[[36m2023-10-28 13:07:46,962[39m][[34mroot[39m][[32mINFO[39m] - Step 58880 @ 0.0 SPS. Inference batcher size: 56. Learner queue size: 32. Other stats: (train_seconds = 581.2, success_rate = 0.0, meta_entropy = tensor(1.3862), hks_loss = 0.017434, step = 58880, mean_episode_return = -0.0034274, mean_episode_step = 15.11, total_loss = -111.63, entropy_loss = -4.0794, pg_loss = -113.98, baseline_loss = 4.2765, learner_queue_size = 32, _tick = 22, _time = 1.6985e+09)
Invalid runs, some did not end in a win or a loss:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
[[36m2023-10-28 13:07:51,972[39m][[34mroot[39m][[32mINFO[39m] - Step 61440 @ 510.8 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (train_seconds = 586.2, success_rate = 0.0, meta_entropy = tensor(1.3862), hks_loss = 0.026131, step = 61440, mean_episode_return = -0.0032553, mean_episode_step = 13.657, total_loss = 84.764, entropy_loss = -4.0728, pg_loss = 82.518, baseline_loss = 3.531, learner_queue_size = 32, _tick = 23, _time = 1.6985e+09)
[[36m2023-10-28 13:07:56,979[39m][[34mroot[39m][[32mINFO[39m] - Step 61440 @ 0.0 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 591.2, success_rate = 0.0, meta_entropy = tensor(1.3862), hks_loss = 0.026131, step = 61440, mean_episode_return = -0.0032553, mean_episode_step = 13.657, total_loss = 84.764, entropy_loss = -4.0728, pg_loss = 82.518, baseline_loss = 3.531, learner_queue_size = 32, _tick = 23, _time = 1.6985e+09)
[[36m2023-10-28 13:08:01,985[39m][[34mroot[39m][[32mINFO[39m] - Step 61440 @ 0.0 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (train_seconds = 596.2, success_rate = 0.0, meta_entropy = tensor(1.3862), hks_loss = 0.026131, step = 61440, mean_episode_return = -0.0032553, mean_episode_step = 13.657, total_loss = 84.764, entropy_loss = -4.0728, pg_loss = 82.518, baseline_loss = 3.531, learner_queue_size = 32, _tick = 23, _time = 1.6985e+09)
~ 64000 total_loss tensor(-72.2368, grad_fn=<AddBackward0>) ks_loss tensor(3.2444, grad_fn=<MulBackward0>)
### 64000 total_loss tensor(-68.9924, grad_fn=<AddBackward0>) hks_uniform_loss tensor(0.0185, grad_fn=<MulBackward0>)
[[36m2023-10-28 13:08:06,992[39m][[34mroot[39m][[32mINFO[39m] - Saving checkpoint to /workspace/outputs/2023-10-28/12-58-00/quest_easy.tar
[[36m2023-10-28 13:08:07,063[39m][[34mroot[39m][[32mINFO[39m] - Step 61440 @ 0.0 SPS. Inference batcher size: 80. Learner queue size: 32. Other stats: (train_seconds = 601.2, success_rate = 0.0, meta_entropy = tensor(1.3862), hks_loss = 0.026131, step = 61440, mean_episode_return = -0.0032553, mean_episode_step = 13.657, total_loss = 84.764, entropy_loss = -4.0728, pg_loss = 82.518, baseline_loss = 3.531, learner_queue_size = 32, _tick = 23, _time = 1.6985e+09)
Invalid runs, some did not end in a win or a loss:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
[[36m2023-10-28 13:08:12,069[39m][[34mroot[39m][[32mINFO[39m] - Step 64000 @ 504.3 SPS. Inference batcher size: 107. Learner queue size: 32. Other stats: (train_seconds = 606.3, success_rate = 0.0, meta_entropy = tensor(1.3863), hks_loss = 0.018486, step = 64000, mean_episode_return = -0.0037197, mean_episode_step = 14.507, total_loss = -68.974, entropy_loss = -4.0658, pg_loss = -71.741, baseline_loss = 3.5703, learner_queue_size = 32, _tick = 24, _time = 1.6985e+09)
[[36m2023-10-28 13:08:17,077[39m][[34mroot[39m][[32mINFO[39m] - Step 64000 @ 0.0 SPS. Inference batcher size: 57. Learner queue size: 32. Other stats: (train_seconds = 611.3, success_rate = 0.0, meta_entropy = tensor(1.3863), hks_loss = 0.018486, step = 64000, mean_episode_return = -0.0037197, mean_episode_step = 14.507, total_loss = -68.974, entropy_loss = -4.0658, pg_loss = -71.741, baseline_loss = 3.5703, learner_queue_size = 32, _tick = 24, _time = 1.6985e+09)
[[36m2023-10-28 13:08:22,085[39m][[34mroot[39m][[32mINFO[39m] - Step 64000 @ 0.0 SPS. Inference batcher size: 89. Learner queue size: 32. Other stats: (train_seconds = 616.3, success_rate = 0.0, meta_entropy = tensor(1.3863), hks_loss = 0.018486, step = 64000, mean_episode_return = -0.0037197, mean_episode_step = 14.507, total_loss = -68.974, entropy_loss = -4.0658, pg_loss = -71.741, baseline_loss = 3.5703, learner_queue_size = 32, _tick = 24, _time = 1.6985e+09)
~ 66560 total_loss tensor(22.8371, grad_fn=<AddBackward0>) ks_loss tensor(3.1700, grad_fn=<MulBackward0>)
### 66560 total_loss tensor(26.0071, grad_fn=<AddBackward0>) hks_uniform_loss tensor(0.0253, grad_fn=<MulBackward0>)
[[36m2023-10-28 13:08:27,089[39m][[34mroot[39m][[32mINFO[39m] - Step 64000 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 621.3, success_rate = 0.0, meta_entropy = tensor(1.3863), hks_loss = 0.018486, step = 64000, mean_episode_return = -0.0037197, mean_episode_step = 14.507, total_loss = -68.974, entropy_loss = -4.0658, pg_loss = -71.741, baseline_loss = 3.5703, learner_queue_size = 32, _tick = 24, _time = 1.6985e+09)
[[36m2023-10-28 13:08:32,097[39m][[34mroot[39m][[32mINFO[39m] - Step 64000 @ 0.0 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (train_seconds = 626.3, success_rate = 0.0, meta_entropy = tensor(1.3863), hks_loss = 0.018486, step = 64000, mean_episode_return = -0.0037197, mean_episode_step = 14.507, total_loss = -68.974, entropy_loss = -4.0658, pg_loss = -71.741, baseline_loss = 3.5703, learner_queue_size = 32, _tick = 24, _time = 1.6985e+09)
Invalid runs, some did not end in a win or a loss:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0.])
[[36m2023-10-28 13:08:37,106[39m][[34mroot[39m][[32mINFO[39m] - Step 66560 @ 511.2 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (train_seconds = 631.3, success_rate = 0.0, meta_entropy = tensor(1.3863), hks_loss = 0.02534, step = 66560, mean_episode_return = -0.0035969, mean_episode_step = 16.625, total_loss = 26.032, entropy_loss = -4.0673, pg_loss = 21.915, baseline_loss = 4.9897, learner_queue_size = 32, _tick = 25, _time = 1.6985e+09)
[[36m2023-10-28 13:08:42,117[39m][[34mroot[39m][[32mINFO[39m] - Step 66560 @ 0.0 SPS. Inference batcher size: 27. Learner queue size: 32. Other stats: (train_seconds = 636.3, success_rate = 0.0, meta_entropy = tensor(1.3863), hks_loss = 0.02534, step = 66560, mean_episode_return = -0.0035969, mean_episode_step = 16.625, total_loss = 26.032, entropy_loss = -4.0673, pg_loss = 21.915, baseline_loss = 4.9897, learner_queue_size = 32, _tick = 25, _time = 1.6985e+09)
[[36m2023-10-28 13:08:47,125[39m][[34mroot[39m][[32mINFO[39m] - Step 66560 @ 0.0 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (train_seconds = 641.3, success_rate = 0.0, meta_entropy = tensor(1.3863), hks_loss = 0.02534, step = 66560, mean_episode_return = -0.0035969, mean_episode_step = 16.625, total_loss = 26.032, entropy_loss = -4.0673, pg_loss = 21.915, baseline_loss = 4.9897, learner_queue_size = 32, _tick = 25, _time = 1.6985e+09)
~ 69120 total_loss tensor(0.1769, grad_fn=<AddBackward0>) ks_loss tensor(4.3253, grad_fn=<MulBackward0>)
### 69120 total_loss tensor(4.5023, grad_fn=<AddBackward0>) hks_uniform_loss tensor(0.0284, grad_fn=<MulBackward0>)
[[36m2023-10-28 13:08:52,134[39m][[34mroot[39m][[32mINFO[39m] - Step 66560 @ 0.0 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (train_seconds = 646.3, success_rate = 0.0, meta_entropy = tensor(1.3863), hks_loss = 0.02534, step = 66560, mean_episode_return = -0.0035969, mean_episode_step = 16.625, total_loss = 26.032, entropy_loss = -4.0673, pg_loss = 21.915, baseline_loss = 4.9897, learner_queue_size = 32, _tick = 25, _time = 1.6985e+09)
Invalid runs, some did not end in a win or a loss:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.])
[[36m2023-10-28 13:08:57,141[39m][[34mroot[39m][[32mINFO[39m] - Step 69120 @ 511.2 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (train_seconds = 651.3, success_rate = 0.0, meta_entropy = tensor(1.3863), hks_loss = 0.028416, step = 69120, mean_episode_return = -0.0034836, mean_episode_step = 15.365, total_loss = 4.5307, entropy_loss = -4.0547, pg_loss = 1.2342, baseline_loss = 2.9974, learner_queue_size = 32, _tick = 26, _time = 1.6985e+09)
[[36m2023-10-28 13:09:02,149[39m][[34mroot[39m][[32mINFO[39m] - Step 69120 @ 0.0 SPS. Inference batcher size: 68. Learner queue size: 32. Other stats: (train_seconds = 656.4, success_rate = 0.0, meta_entropy = tensor(1.3863), hks_loss = 0.028416, step = 69120, mean_episode_return = -0.0034836, mean_episode_step = 15.365, total_loss = 4.5307, entropy_loss = -4.0547, pg_loss = 1.2342, baseline_loss = 2.9974, learner_queue_size = 32, _tick = 26, _time = 1.6985e+09)
[[36m2023-10-28 13:09:07,157[39m][[34mroot[39m][[32mINFO[39m] - Step 69120 @ 0.0 SPS. Inference batcher size: 37. Learner queue size: 32. Other stats: (train_seconds = 661.4, success_rate = 0.0, meta_entropy = tensor(1.3863), hks_loss = 0.028416, step = 69120, mean_episode_return = -0.0034836, mean_episode_step = 15.365, total_loss = 4.5307, entropy_loss = -4.0547, pg_loss = 1.2342, baseline_loss = 2.9974, learner_queue_size = 32, _tick = 26, _time = 1.6985e+09)
~ 71680 total_loss tensor(18.8418, grad_fn=<AddBackward0>) ks_loss tensor(3.7546, grad_fn=<MulBackward0>)
### 71680 total_loss tensor(22.5964, grad_fn=<AddBackward0>) hks_uniform_loss tensor(0.0257, grad_fn=<MulBackward0>)
[[36m2023-10-28 13:09:12,164[39m][[34mroot[39m][[32mINFO[39m] - Step 69120 @ 0.0 SPS. Inference batcher size: 70. Learner queue size: 32. Other stats: (train_seconds = 666.4, success_rate = 0.0, meta_entropy = tensor(1.3863), hks_loss = 0.028416, step = 69120, mean_episode_return = -0.0034836, mean_episode_step = 15.365, total_loss = 4.5307, entropy_loss = -4.0547, pg_loss = 1.2342, baseline_loss = 2.9974, learner_queue_size = 32, _tick = 26, _time = 1.6985e+09)
Invalid runs, some did not end in a win or a loss:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
[[36m2023-10-28 13:09:17,170[39m][[34mroot[39m][[32mINFO[39m] - Step 71680 @ 511.6 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (train_seconds = 671.4, success_rate = 0.0, meta_entropy = tensor(1.3863), hks_loss = 0.025695, step = 71680, mean_episode_return = -0.0034091, mean_episode_step = 15.084, total_loss = 22.622, entropy_loss = -4.0614, pg_loss = 20.325, baseline_loss = 2.5786, learner_queue_size = 32, _tick = 27, _time = 1.6985e+09)
[[36m2023-10-28 13:09:22,182[39m][[34mroot[39m][[32mINFO[39m] - Step 71680 @ 0.0 SPS. Inference batcher size: 121. Learner queue size: 32. Other stats: (train_seconds = 676.4, success_rate = 0.0, meta_entropy = tensor(1.3863), hks_loss = 0.025695, step = 71680, mean_episode_return = -0.0034091, mean_episode_step = 15.084, total_loss = 22.622, entropy_loss = -4.0614, pg_loss = 20.325, baseline_loss = 2.5786, learner_queue_size = 32, _tick = 27, _time = 1.6985e+09)
[[36m2023-10-28 13:09:27,189[39m][[34mroot[39m][[32mINFO[39m] - Step 71680 @ 0.0 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (train_seconds = 681.4, success_rate = 0.0, meta_entropy = tensor(1.3863), hks_loss = 0.025695, step = 71680, mean_episode_return = -0.0034091, mean_episode_step = 15.084, total_loss = 22.622, entropy_loss = -4.0614, pg_loss = 20.325, baseline_loss = 2.5786, learner_queue_size = 32, _tick = 27, _time = 1.6985e+09)
~ 74240 total_loss tensor(-33.7224, grad_fn=<AddBackward0>) ks_loss tensor(3.7601, grad_fn=<MulBackward0>)
### 74240 total_loss tensor(-29.9623, grad_fn=<AddBackward0>) hks_uniform_loss tensor(0.0234, grad_fn=<MulBackward0>)
[[36m2023-10-28 13:09:32,192[39m][[34mroot[39m][[32mINFO[39m] - Step 71680 @ 0.0 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (train_seconds = 686.4, success_rate = 0.0, meta_entropy = tensor(1.3863), hks_loss = 0.025695, step = 71680, mean_episode_return = -0.0034091, mean_episode_step = 15.084, total_loss = 22.622, entropy_loss = -4.0614, pg_loss = 20.325, baseline_loss = 2.5786, learner_queue_size = 32, _tick = 27, _time = 1.6985e+09)
[[36m2023-10-28 13:09:37,196[39m][[34mroot[39m][[32mINFO[39m] - Step 71680 @ 0.0 SPS. Inference batcher size: 47. Learner queue size: 32. Other stats: (train_seconds = 691.4, success_rate = 0.0, meta_entropy = tensor(1.3863), hks_loss = 0.025695, step = 71680, mean_episode_return = -0.0034091, mean_episode_step = 15.084, total_loss = 22.622, entropy_loss = -4.0614, pg_loss = 20.325, baseline_loss = 2.5786, learner_queue_size = 32, _tick = 27, _time = 1.6985e+09)
Invalid runs, some did not end in a win or a loss:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
[[36m2023-10-28 13:09:42,201[39m][[34mroot[39m][[32mINFO[39m] - Step 74240 @ 511.6 SPS. Inference batcher size: 24. Learner queue size: 32. Other stats: (train_seconds = 696.4, success_rate = 0.0, meta_entropy = tensor(1.3862), hks_loss = 0.023435, step = 74240, mean_episode_return = -0.0030507, mean_episode_step = 13.955, total_loss = -29.939, entropy_loss = -4.0616, pg_loss = -32.754, baseline_loss = 3.0935, learner_queue_size = 32, _tick = 28, _time = 1.6985e+09)
[[36m2023-10-28 13:09:47,210[39m][[34mroot[39m][[32mINFO[39m] - Step 74240 @ 0.0 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 701.4, success_rate = 0.0, meta_entropy = tensor(1.3862), hks_loss = 0.023435, step = 74240, mean_episode_return = -0.0030507, mean_episode_step = 13.955, total_loss = -29.939, entropy_loss = -4.0616, pg_loss = -32.754, baseline_loss = 3.0935, learner_queue_size = 32, _tick = 28, _time = 1.6985e+09)
[[36m2023-10-28 13:09:52,216[39m][[34mroot[39m][[32mINFO[39m] - Step 74240 @ 0.0 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (train_seconds = 706.4, success_rate = 0.0, meta_entropy = tensor(1.3862), hks_loss = 0.023435, step = 74240, mean_episode_return = -0.0030507, mean_episode_step = 13.955, total_loss = -29.939, entropy_loss = -4.0616, pg_loss = -32.754, baseline_loss = 3.0935, learner_queue_size = 32, _tick = 28, _time = 1.6985e+09)
~ 76800 total_loss tensor(55.2096, grad_fn=<AddBackward0>) ks_loss tensor(3.5549, grad_fn=<MulBackward0>)
### 76800 total_loss tensor(58.7645, grad_fn=<AddBackward0>) hks_uniform_loss tensor(0.0334, grad_fn=<MulBackward0>)
[[36m2023-10-28 13:09:57,222[39m][[34mroot[39m][[32mINFO[39m] - Step 74240 @ 0.0 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (train_seconds = 711.4, success_rate = 0.0, meta_entropy = tensor(1.3862), hks_loss = 0.023435, step = 74240, mean_episode_return = -0.0030507, mean_episode_step = 13.955, total_loss = -29.939, entropy_loss = -4.0616, pg_loss = -32.754, baseline_loss = 3.0935, learner_queue_size = 32, _tick = 28, _time = 1.6985e+09)
Invalid runs, some did not end in a win or a loss:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
[[36m2023-10-28 13:10:02,229[39m][[34mroot[39m][[32mINFO[39m] - Saving checkpoint to /workspace/outputs/2023-10-28/12-58-00/quest_easy_0.75.tar
[[36m2023-10-28 13:10:02,275[39m][[34mroot[39m][[32mINFO[39m] - Step 76800 @ 511.3 SPS. Inference batcher size: 22. Learner queue size: 32. Other stats: (train_seconds = 716.4, success_rate = 0.0, meta_entropy = tensor(1.3862), hks_loss = 0.033369, step = 76800, mean_episode_return = -0.0030149, mean_episode_step = 13.718, total_loss = 58.798, entropy_loss = -4.0639, pg_loss = 56.238, baseline_loss = 3.0356, learner_queue_size = 32, _tick = 29, _time = 1.6985e+09)
[[36m2023-10-28 13:10:07,284[39m][[34mroot[39m][[32mINFO[39m] - Step 76800 @ 0.0 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 721.5, success_rate = 0.0, meta_entropy = tensor(1.3862), hks_loss = 0.033369, step = 76800, mean_episode_return = -0.0030149, mean_episode_step = 13.718, total_loss = 58.798, entropy_loss = -4.0639, pg_loss = 56.238, baseline_loss = 3.0356, learner_queue_size = 32, _tick = 29, _time = 1.6985e+09)
[[36m2023-10-28 13:10:12,289[39m][[34mroot[39m][[32mINFO[39m] - Step 76800 @ 0.0 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (train_seconds = 726.5, success_rate = 0.0, meta_entropy = tensor(1.3862), hks_loss = 0.033369, step = 76800, mean_episode_return = -0.0030149, mean_episode_step = 13.718, total_loss = 58.798, entropy_loss = -4.0639, pg_loss = 56.238, baseline_loss = 3.0356, learner_queue_size = 32, _tick = 29, _time = 1.6985e+09)
~ 79360 total_loss tensor(-56.6656, grad_fn=<AddBackward0>) ks_loss tensor(3.0078, grad_fn=<MulBackward0>)
### 79360 total_loss tensor(-53.6577, grad_fn=<AddBackward0>) hks_uniform_loss tensor(0.0181, grad_fn=<MulBackward0>)
[[36m2023-10-28 13:10:17,297[39m][[34mroot[39m][[32mINFO[39m] - Step 76800 @ 0.0 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 731.5, success_rate = 0.0, meta_entropy = tensor(1.3862), hks_loss = 0.033369, step = 76800, mean_episode_return = -0.0030149, mean_episode_step = 13.718, total_loss = 58.798, entropy_loss = -4.0639, pg_loss = 56.238, baseline_loss = 3.0356, learner_queue_size = 32, _tick = 29, _time = 1.6985e+09)
Invalid runs, some did not end in a win or a loss:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.])
[[36m2023-10-28 13:10:22,306[39m][[34mroot[39m][[32mINFO[39m] - Step 79360 @ 511.2 SPS. Inference batcher size: 50. Learner queue size: 32. Other stats: (train_seconds = 736.5, success_rate = 0.0, meta_entropy = tensor(1.3862), hks_loss = 0.018086, step = 79360, mean_episode_return = -0.0032973, mean_episode_step = 12.286, total_loss = -53.64, entropy_loss = -4.07, pg_loss = -55.83, baseline_loss = 3.2347, learner_queue_size = 32, _tick = 30, _time = 1.6985e+09)
[[36m2023-10-28 13:10:27,318[39m][[34mroot[39m][[32mINFO[39m] - Step 79360 @ 0.0 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (train_seconds = 741.5, success_rate = 0.0, meta_entropy = tensor(1.3862), hks_loss = 0.018086, step = 79360, mean_episode_return = -0.0032973, mean_episode_step = 12.286, total_loss = -53.64, entropy_loss = -4.07, pg_loss = -55.83, baseline_loss = 3.2347, learner_queue_size = 32, _tick = 30, _time = 1.6985e+09)
[[36m2023-10-28 13:10:32,325[39m][[34mroot[39m][[32mINFO[39m] - Step 79360 @ 0.0 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (train_seconds = 746.5, success_rate = 0.0, meta_entropy = tensor(1.3862), hks_loss = 0.018086, step = 79360, mean_episode_return = -0.0032973, mean_episode_step = 12.286, total_loss = -53.64, entropy_loss = -4.07, pg_loss = -55.83, baseline_loss = 3.2347, learner_queue_size = 32, _tick = 30, _time = 1.6985e+09)
~ 81920 total_loss tensor(-22.1163, grad_fn=<AddBackward0>) ks_loss tensor(3.5090, grad_fn=<MulBackward0>)
### 81920 total_loss tensor(-18.6074, grad_fn=<AddBackward0>) hks_uniform_loss tensor(0.0267, grad_fn=<MulBackward0>)
[[36m2023-10-28 13:10:37,328[39m][[34mroot[39m][[32mINFO[39m] - Step 79360 @ 0.0 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (train_seconds = 751.5, success_rate = 0.0, meta_entropy = tensor(1.3862), hks_loss = 0.018086, step = 79360, mean_episode_return = -0.0032973, mean_episode_step = 12.286, total_loss = -53.64, entropy_loss = -4.07, pg_loss = -55.83, baseline_loss = 3.2347, learner_queue_size = 32, _tick = 30, _time = 1.6985e+09)
[[36m2023-10-28 13:10:42,333[39m][[34mroot[39m][[32mINFO[39m] - Step 79360 @ 0.0 SPS. Inference batcher size: 23. Learner queue size: 32. Other stats: (train_seconds = 756.5, success_rate = 0.0, meta_entropy = tensor(1.3862), hks_loss = 0.018086, step = 79360, mean_episode_return = -0.0032973, mean_episode_step = 12.286, total_loss = -53.64, entropy_loss = -4.07, pg_loss = -55.83, baseline_loss = 3.2347, learner_queue_size = 32, _tick = 30, _time = 1.6985e+09)
Invalid runs, some did not end in a win or a loss:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
[[36m2023-10-28 13:10:47,341[39m][[34mroot[39m][[32mINFO[39m] - Step 81920 @ 511.2 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (train_seconds = 761.5, success_rate = 0.0, meta_entropy = tensor(1.3862), hks_loss = 0.026712, step = 81920, mean_episode_return = -0.0031353, mean_episode_step = 15.289, total_loss = -18.581, entropy_loss = -4.0642, pg_loss = -22.025, baseline_loss = 3.9728, learner_queue_size = 32, _tick = 31, _time = 1.6985e+09)
[[36m2023-10-28 13:10:52,348[39m][[34mroot[39m][[32mINFO[39m] - Step 81920 @ 0.0 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 766.6, success_rate = 0.0, meta_entropy = tensor(1.3862), hks_loss = 0.026712, step = 81920, mean_episode_return = -0.0031353, mean_episode_step = 15.289, total_loss = -18.581, entropy_loss = -4.0642, pg_loss = -22.025, baseline_loss = 3.9728, learner_queue_size = 32, _tick = 31, _time = 1.6985e+09)
[[36m2023-10-28 13:10:57,352[39m][[34mroot[39m][[32mINFO[39m] - Step 81920 @ 0.0 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 771.6, success_rate = 0.0, meta_entropy = tensor(1.3862), hks_loss = 0.026712, step = 81920, mean_episode_return = -0.0031353, mean_episode_step = 15.289, total_loss = -18.581, entropy_loss = -4.0642, pg_loss = -22.025, baseline_loss = 3.9728, learner_queue_size = 32, _tick = 31, _time = 1.6985e+09)
~ 84480 total_loss tensor(64.9480, grad_fn=<AddBackward0>) ks_loss tensor(4.1394, grad_fn=<MulBackward0>)
### 84480 total_loss tensor(69.0874, grad_fn=<AddBackward0>) hks_uniform_loss tensor(0.0337, grad_fn=<MulBackward0>)
[[36m2023-10-28 13:11:02,358[39m][[34mroot[39m][[32mINFO[39m] - Step 81920 @ 0.0 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (train_seconds = 776.6, success_rate = 0.0, meta_entropy = tensor(1.3862), hks_loss = 0.026712, step = 81920, mean_episode_return = -0.0031353, mean_episode_step = 15.289, total_loss = -18.581, entropy_loss = -4.0642, pg_loss = -22.025, baseline_loss = 3.9728, learner_queue_size = 32, _tick = 31, _time = 1.6985e+09)
Invalid runs, some did not end in a win or a loss:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
[[36m2023-10-28 13:11:07,369[39m][[34mroot[39m][[32mINFO[39m] - Step 84480 @ 510.8 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 781.6, success_rate = 0.0, meta_entropy = tensor(1.3862), hks_loss = 0.033744, step = 84480, mean_episode_return = -0.0031522, mean_episode_step = 13.936, total_loss = 69.121, entropy_loss = -4.0573, pg_loss = 66.213, baseline_loss = 2.7923, learner_queue_size = 32, _tick = 32, _time = 1.6985e+09)
[[36m2023-10-28 13:11:12,377[39m][[34mroot[39m][[32mINFO[39m] - Step 84480 @ 0.0 SPS. Inference batcher size: 66. Learner queue size: 32. Other stats: (train_seconds = 786.6, success_rate = 0.0, meta_entropy = tensor(1.3862), hks_loss = 0.033744, step = 84480, mean_episode_return = -0.0031522, mean_episode_step = 13.936, total_loss = 69.121, entropy_loss = -4.0573, pg_loss = 66.213, baseline_loss = 2.7923, learner_queue_size = 32, _tick = 32, _time = 1.6985e+09)
[[36m2023-10-28 13:11:17,386[39m][[34mroot[39m][[32mINFO[39m] - Step 84480 @ 0.0 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (train_seconds = 791.6, success_rate = 0.0, meta_entropy = tensor(1.3862), hks_loss = 0.033744, step = 84480, mean_episode_return = -0.0031522, mean_episode_step = 13.936, total_loss = 69.121, entropy_loss = -4.0573, pg_loss = 66.213, baseline_loss = 2.7923, learner_queue_size = 32, _tick = 32, _time = 1.6985e+09)
~ 87040 total_loss tensor(-14.8377, grad_fn=<AddBackward0>) ks_loss tensor(3.5316, grad_fn=<MulBackward0>)
### 87040 total_loss tensor(-11.3061, grad_fn=<AddBackward0>) hks_uniform_loss tensor(0.0246, grad_fn=<MulBackward0>)
[[36m2023-10-28 13:11:22,392[39m][[34mroot[39m][[32mINFO[39m] - Step 84480 @ 0.0 SPS. Inference batcher size: 86. Learner queue size: 32. Other stats: (train_seconds = 796.6, success_rate = 0.0, meta_entropy = tensor(1.3862), hks_loss = 0.033744, step = 84480, mean_episode_return = -0.0031522, mean_episode_step = 13.936, total_loss = 69.121, entropy_loss = -4.0573, pg_loss = 66.213, baseline_loss = 2.7923, learner_queue_size = 32, _tick = 32, _time = 1.6985e+09)
[[36m2023-10-28 13:11:27,398[39m][[34mroot[39m][[32mINFO[39m] - Step 84480 @ 0.0 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (train_seconds = 801.6, success_rate = 0.0, meta_entropy = tensor(1.3862), hks_loss = 0.033744, step = 84480, mean_episode_return = -0.0031522, mean_episode_step = 13.936, total_loss = 69.121, entropy_loss = -4.0573, pg_loss = 66.213, baseline_loss = 2.7923, learner_queue_size = 32, _tick = 32, _time = 1.6985e+09)
Invalid runs, some did not end in a win or a loss:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.])
[[36m2023-10-28 13:11:32,408[39m][[34mroot[39m][[32mINFO[39m] - Step 87040 @ 510.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (train_seconds = 806.6, success_rate = 0.0, meta_entropy = tensor(1.3862), hks_loss = 0.024593, step = 87040, mean_episode_return = -0.0029726, mean_episode_step = 13.421, total_loss = -11.282, entropy_loss = -4.0648, pg_loss = -13.681, baseline_loss = 2.9077, learner_queue_size = 32, _tick = 33, _time = 1.6985e+09)
[[36m2023-10-28 13:11:37,413[39m][[34mroot[39m][[32mINFO[39m] - Step 87040 @ 0.0 SPS. Inference batcher size: 39. Learner queue size: 32. Other stats: (train_seconds = 811.6, success_rate = 0.0, meta_entropy = tensor(1.3862), hks_loss = 0.024593, step = 87040, mean_episode_return = -0.0029726, mean_episode_step = 13.421, total_loss = -11.282, entropy_loss = -4.0648, pg_loss = -13.681, baseline_loss = 2.9077, learner_queue_size = 32, _tick = 33, _time = 1.6985e+09)
[[36m2023-10-28 13:11:42,419[39m][[34mroot[39m][[32mINFO[39m] - Step 87040 @ 0.0 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (train_seconds = 816.6, success_rate = 0.0, meta_entropy = tensor(1.3862), hks_loss = 0.024593, step = 87040, mean_episode_return = -0.0029726, mean_episode_step = 13.421, total_loss = -11.282, entropy_loss = -4.0648, pg_loss = -13.681, baseline_loss = 2.9077, learner_queue_size = 32, _tick = 33, _time = 1.6985e+09)
~ 89600 total_loss tensor(36.9431, grad_fn=<AddBackward0>) ks_loss tensor(3.4506, grad_fn=<MulBackward0>)
### 89600 total_loss tensor(40.3936, grad_fn=<AddBackward0>) hks_uniform_loss tensor(0.0264, grad_fn=<MulBackward0>)
[[36m2023-10-28 13:11:47,424[39m][[34mroot[39m][[32mINFO[39m] - Step 87040 @ 0.0 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 821.6, success_rate = 0.0, meta_entropy = tensor(1.3862), hks_loss = 0.024593, step = 87040, mean_episode_return = -0.0029726, mean_episode_step = 13.421, total_loss = -11.282, entropy_loss = -4.0648, pg_loss = -13.681, baseline_loss = 2.9077, learner_queue_size = 32, _tick = 33, _time = 1.6985e+09)
Invalid runs, some did not end in a win or a loss:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0.])
[[36m2023-10-28 13:11:52,430[39m][[34mroot[39m][[32mINFO[39m] - Step 89600 @ 511.6 SPS. Inference batcher size: 60. Learner queue size: 32. Other stats: (train_seconds = 826.6, success_rate = 0.0, meta_entropy = tensor(1.3862), hks_loss = 0.026375, step = 89600, mean_episode_return = -0.0028658, mean_episode_step = 13.068, total_loss = 40.42, entropy_loss = -4.0656, pg_loss = 38.701, baseline_loss = 2.3079, learner_queue_size = 32, _tick = 34, _time = 1.6985e+09)
[[36m2023-10-28 13:11:57,437[39m][[34mroot[39m][[32mINFO[39m] - Step 89600 @ 0.0 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (train_seconds = 831.6, success_rate = 0.0, meta_entropy = tensor(1.3862), hks_loss = 0.026375, step = 89600, mean_episode_return = -0.0028658, mean_episode_step = 13.068, total_loss = 40.42, entropy_loss = -4.0656, pg_loss = 38.701, baseline_loss = 2.3079, learner_queue_size = 32, _tick = 34, _time = 1.6985e+09)
[[36m2023-10-28 13:12:02,441[39m][[34mroot[39m][[32mINFO[39m] - Step 89600 @ 0.0 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (train_seconds = 836.6, success_rate = 0.0, meta_entropy = tensor(1.3862), hks_loss = 0.026375, step = 89600, mean_episode_return = -0.0028658, mean_episode_step = 13.068, total_loss = 40.42, entropy_loss = -4.0656, pg_loss = 38.701, baseline_loss = 2.3079, learner_queue_size = 32, _tick = 34, _time = 1.6985e+09)
~ 92160 total_loss tensor(-32.2603, grad_fn=<AddBackward0>) ks_loss tensor(3.2285, grad_fn=<MulBackward0>)
### 92160 total_loss tensor(-29.0317, grad_fn=<AddBackward0>) hks_uniform_loss tensor(0.0230, grad_fn=<MulBackward0>)
[[36m2023-10-28 13:12:07,449[39m][[34mroot[39m][[32mINFO[39m] - Step 89600 @ 0.0 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 841.7, success_rate = 0.0, meta_entropy = tensor(1.3862), hks_loss = 0.026375, step = 89600, mean_episode_return = -0.0028658, mean_episode_step = 13.068, total_loss = 40.42, entropy_loss = -4.0656, pg_loss = 38.701, baseline_loss = 2.3079, learner_queue_size = 32, _tick = 34, _time = 1.6985e+09)
Invalid runs, some did not end in a win or a loss:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
[[36m2023-10-28 13:12:12,457[39m][[34mroot[39m][[32mINFO[39m] - Step 92160 @ 511.2 SPS. Inference batcher size: 22. Learner queue size: 32. Other stats: (train_seconds = 846.7, success_rate = 0.0, meta_entropy = tensor(1.3862), hks_loss = 0.022956, step = 92160, mean_episode_return = -0.0031493, mean_episode_step = 14.815, total_loss = -29.009, entropy_loss = -4.0679, pg_loss = -30.862, baseline_loss = 2.6701, learner_queue_size = 32, _tick = 35, _time = 1.6985e+09)
[[36m2023-10-28 13:12:17,466[39m][[34mroot[39m][[32mINFO[39m] - Step 92160 @ 0.0 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (train_seconds = 851.7, success_rate = 0.0, meta_entropy = tensor(1.3862), hks_loss = 0.022956, step = 92160, mean_episode_return = -0.0031493, mean_episode_step = 14.815, total_loss = -29.009, entropy_loss = -4.0679, pg_loss = -30.862, baseline_loss = 2.6701, learner_queue_size = 32, _tick = 35, _time = 1.6985e+09)
[[36m2023-10-28 13:12:22,472[39m][[34mroot[39m][[32mINFO[39m] - Step 92160 @ 0.0 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 856.7, success_rate = 0.0, meta_entropy = tensor(1.3862), hks_loss = 0.022956, step = 92160, mean_episode_return = -0.0031493, mean_episode_step = 14.815, total_loss = -29.009, entropy_loss = -4.0679, pg_loss = -30.862, baseline_loss = 2.6701, learner_queue_size = 32, _tick = 35, _time = 1.6985e+09)
~ 94720 total_loss tensor(-56.6336, grad_fn=<AddBackward0>) ks_loss tensor(3.2701, grad_fn=<MulBackward0>)
### 94720 total_loss tensor(-53.3635, grad_fn=<AddBackward0>) hks_uniform_loss tensor(0.0254, grad_fn=<MulBackward0>)
[[36m2023-10-28 13:12:27,477[39m][[34mroot[39m][[32mINFO[39m] - Step 92160 @ 0.0 SPS. Inference batcher size: 81. Learner queue size: 32. Other stats: (train_seconds = 861.7, success_rate = 0.0, meta_entropy = tensor(1.3862), hks_loss = 0.022956, step = 92160, mean_episode_return = -0.0031493, mean_episode_step = 14.815, total_loss = -29.009, entropy_loss = -4.0679, pg_loss = -30.862, baseline_loss = 2.6701, learner_queue_size = 32, _tick = 35, _time = 1.6985e+09)
[[36m2023-10-28 13:12:32,480[39m][[34mroot[39m][[32mINFO[39m] - Step 92160 @ 0.0 SPS. Inference batcher size: 68. Learner queue size: 32. Other stats: (train_seconds = 866.7, success_rate = 0.0, meta_entropy = tensor(1.3862), hks_loss = 0.022956, step = 92160, mean_episode_return = -0.0031493, mean_episode_step = 14.815, total_loss = -29.009, entropy_loss = -4.0679, pg_loss = -30.862, baseline_loss = 2.6701, learner_queue_size = 32, _tick = 35, _time = 1.6985e+09)
Invalid runs, some did not end in a win or a loss:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
[[36m2023-10-28 13:12:37,484[39m][[34mroot[39m][[32mINFO[39m] - Step 94720 @ 511.6 SPS. Inference batcher size: 84. Learner queue size: 32. Other stats: (train_seconds = 871.7, success_rate = 0.0, meta_entropy = tensor(1.3862), hks_loss = 0.025443, step = 94720, mean_episode_return = -0.0031259, mean_episode_step = 15.109, total_loss = -53.338, entropy_loss = -4.0673, pg_loss = -56.984, baseline_loss = 4.4179, learner_queue_size = 32, _tick = 36, _time = 1.6985e+09)
[[36m2023-10-28 13:12:42,494[39m][[34mroot[39m][[32mINFO[39m] - Step 94720 @ 0.0 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (train_seconds = 876.7, success_rate = 0.0, meta_entropy = tensor(1.3862), hks_loss = 0.025443, step = 94720, mean_episode_return = -0.0031259, mean_episode_step = 15.109, total_loss = -53.338, entropy_loss = -4.0673, pg_loss = -56.984, baseline_loss = 4.4179, learner_queue_size = 32, _tick = 36, _time = 1.6985e+09)
[[36m2023-10-28 13:12:47,501[39m][[34mroot[39m][[32mINFO[39m] - Step 94720 @ 0.0 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 881.7, success_rate = 0.0, meta_entropy = tensor(1.3862), hks_loss = 0.025443, step = 94720, mean_episode_return = -0.0031259, mean_episode_step = 15.109, total_loss = -53.338, entropy_loss = -4.0673, pg_loss = -56.984, baseline_loss = 4.4179, learner_queue_size = 32, _tick = 36, _time = 1.6985e+09)
~ 97280 total_loss tensor(14.4129, grad_fn=<AddBackward0>) ks_loss tensor(3.6623, grad_fn=<MulBackward0>)
### 97280 total_loss tensor(18.0752, grad_fn=<AddBackward0>) hks_uniform_loss tensor(0.0307, grad_fn=<MulBackward0>)
[[36m2023-10-28 13:12:52,505[39m][[34mroot[39m][[32mINFO[39m] - Step 94720 @ 0.0 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (train_seconds = 886.7, success_rate = 0.0, meta_entropy = tensor(1.3862), hks_loss = 0.025443, step = 94720, mean_episode_return = -0.0031259, mean_episode_step = 15.109, total_loss = -53.338, entropy_loss = -4.0673, pg_loss = -56.984, baseline_loss = 4.4179, learner_queue_size = 32, _tick = 36, _time = 1.6985e+09)
Invalid runs, some did not end in a win or a loss:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
[[36m2023-10-28 13:12:57,509[39m][[34mroot[39m][[32mINFO[39m] - Step 97280 @ 511.6 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (train_seconds = 891.7, success_rate = 0.0, meta_entropy = tensor(1.3862), hks_loss = 0.030702, step = 97280, mean_episode_return = -0.0029574, mean_episode_step = 14.276, total_loss = 18.106, entropy_loss = -4.0629, pg_loss = 15.106, baseline_loss = 3.3694, learner_queue_size = 32, _tick = 37, _time = 1.6985e+09)
[[36m2023-10-28 13:13:02,518[39m][[34mroot[39m][[32mINFO[39m] - Step 97280 @ 0.0 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (train_seconds = 896.7, success_rate = 0.0, meta_entropy = tensor(1.3862), hks_loss = 0.030702, step = 97280, mean_episode_return = -0.0029574, mean_episode_step = 14.276, total_loss = 18.106, entropy_loss = -4.0629, pg_loss = 15.106, baseline_loss = 3.3694, learner_queue_size = 32, _tick = 37, _time = 1.6985e+09)
[[36m2023-10-28 13:13:07,525[39m][[34mroot[39m][[32mINFO[39m] - Step 97280 @ 0.0 SPS. Inference batcher size: 25. Learner queue size: 32. Other stats: (train_seconds = 901.7, success_rate = 0.0, meta_entropy = tensor(1.3862), hks_loss = 0.030702, step = 97280, mean_episode_return = -0.0029574, mean_episode_step = 14.276, total_loss = 18.106, entropy_loss = -4.0629, pg_loss = 15.106, baseline_loss = 3.3694, learner_queue_size = 32, _tick = 37, _time = 1.6985e+09)
~ 99840 total_loss tensor(39.1647, grad_fn=<AddBackward0>) ks_loss tensor(3.6366, grad_fn=<MulBackward0>)
### 99840 total_loss tensor(42.8013, grad_fn=<AddBackward0>) hks_uniform_loss tensor(0.0292, grad_fn=<MulBackward0>)
[[36m2023-10-28 13:13:12,533[39m][[34mroot[39m][[32mINFO[39m] - Step 97280 @ 0.0 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (train_seconds = 906.7, success_rate = 0.0, meta_entropy = tensor(1.3862), hks_loss = 0.030702, step = 97280, mean_episode_return = -0.0029574, mean_episode_step = 14.276, total_loss = 18.106, entropy_loss = -4.0629, pg_loss = 15.106, baseline_loss = 3.3694, learner_queue_size = 32, _tick = 37, _time = 1.6985e+09)
Invalid runs, some did not end in a win or a loss:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.])
[[36m2023-10-28 13:13:17,541[39m][[34mroot[39m][[32mINFO[39m] - Step 99840 @ 511.2 SPS. Inference batcher size: 77. Learner queue size: 32. Other stats: (train_seconds = 911.7, success_rate = 0.0, meta_entropy = tensor(1.3862), hks_loss = 0.029226, step = 99840, mean_episode_return = -0.0027793, mean_episode_step = 12.558, total_loss = 42.831, entropy_loss = -4.0633, pg_loss = 40.806, baseline_loss = 2.4216, learner_queue_size = 32, _tick = 38, _time = 1.6985e+09)
[[36m2023-10-28 13:13:22,549[39m][[34mroot[39m][[32mINFO[39m] - Step 99840 @ 0.0 SPS. Inference batcher size: 98. Learner queue size: 32. Other stats: (train_seconds = 916.8, success_rate = 0.0, meta_entropy = tensor(1.3862), hks_loss = 0.029226, step = 99840, mean_episode_return = -0.0027793, mean_episode_step = 12.558, total_loss = 42.831, entropy_loss = -4.0633, pg_loss = 40.806, baseline_loss = 2.4216, learner_queue_size = 32, _tick = 38, _time = 1.6985e+09)
[[36m2023-10-28 13:13:27,556[39m][[34mroot[39m][[32mINFO[39m] - Step 99840 @ 0.0 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 921.8, success_rate = 0.0, meta_entropy = tensor(1.3862), hks_loss = 0.029226, step = 99840, mean_episode_return = -0.0027793, mean_episode_step = 12.558, total_loss = 42.831, entropy_loss = -4.0633, pg_loss = 40.806, baseline_loss = 2.4216, learner_queue_size = 32, _tick = 38, _time = 1.6985e+09)
~ 102400 total_loss tensor(36.1967, grad_fn=<AddBackward0>) ks_loss tensor(3.5214, grad_fn=<MulBackward0>)
### 102400 total_loss tensor(39.7181, grad_fn=<AddBackward0>) hks_uniform_loss tensor(0.0271, grad_fn=<MulBackward0>)
[[36m2023-10-28 13:13:32,561[39m][[34mroot[39m][[32mINFO[39m] - Step 99840 @ 0.0 SPS. Inference batcher size: 57. Learner queue size: 32. Other stats: (train_seconds = 926.8, success_rate = 0.0, meta_entropy = tensor(1.3862), hks_loss = 0.029226, step = 99840, mean_episode_return = -0.0027793, mean_episode_step = 12.558, total_loss = 42.831, entropy_loss = -4.0633, pg_loss = 40.806, baseline_loss = 2.4216, learner_queue_size = 32, _tick = 38, _time = 1.6985e+09)
Invalid runs, some did not end in a win or a loss:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
[[36m2023-10-28 13:13:37,564[39m][[34mroot[39m][[32mINFO[39m] - Step 102400 @ 511.7 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 931.8, success_rate = 0.0, meta_entropy = tensor(1.3862), hks_loss = 0.027074, step = 102400, mean_episode_return = -0.0023896, mean_episode_step = 12.39, total_loss = 39.745, entropy_loss = -4.0645, pg_loss = 37.385, baseline_loss = 2.8761, learner_queue_size = 32, _tick = 39, _time = 1.6985e+09)
[[36m2023-10-28 13:13:37,566[39m][[34mroot[39m][[32mINFO[39m] - Learning finished after 102400 steps.
[[36m2023-10-28 13:13:37,566[39m][[34mroot[39m][[32mINFO[39m] - Saving checkpoint to /workspace/outputs/2023-10-28/12-58-00/quest_easy.tar
~ 104960 total_loss tensor(0.9684, grad_fn=<AddBackward0>) ks_loss tensor(3.4010, grad_fn=<MulBackward0>)
### 104960 total_loss tensor(4.3694, grad_fn=<AddBackward0>) hks_uniform_loss tensor(0.0260, grad_fn=<MulBackward0>)
Invalid runs, some did not end in a win or a loss:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0.])